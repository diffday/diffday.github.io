<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>大语言模型LLM | Diffday</title><meta name="keywords" content="技术"><meta name="author" content="DiffDay"><meta name="copyright" content="DiffDay"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="大语言模型LLM"><meta name="application-name" content="大语言模型LLM"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="大语言模型LLM"><meta property="og:url" content="https://blog.diffday.com/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html"><meta property="og:site_name" content="Diffday"><meta property="og:description" content="ChatGPT  GPT:Generative Pre-trained Transformer 科技部长的金句：踢足球都是盘点，射门，但是要做到梅西那么好也不容易  惊喜与惊醒 大语言模型的效果好到令人咋舌，我们距离LLM的认知和发展理念，距离世界最先进的想法，差得有点远  Bert出现后1~2年"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://cdn.diffday.com/picgo/20230408222056.png"><meta property="article:author" content="DiffDay"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://cdn.diffday.com/picgo/20230408222056.png"><meta name="description" content="ChatGPT  GPT:Generative Pre-trained Transformer 科技部长的金句：踢足球都是盘点，射门，但是要做到梅西那么好也不容易  惊喜与惊醒 大语言模型的效果好到令人咋舌，我们距离LLM的认知和发展理念，距离世界最先进的想法，差得有点远  Bert出现后1~2年"><link rel="shortcut icon" href="https://cdn.diffday.com/picgo/diff32.ico"><link rel="canonical" href="https://blog.diffday.com/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><link rel="preconnect" href="//npm.elemecdn.com"/><link rel="preconnect" href="//npm.onmicrosoft.cn"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="UaMvLIOYuNWHgVq42dlN49RoniU73U6SoqoRnEcit9E"/><meta name="baidu-site-verification" content="code-DN25SBuR8T"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b5c9889dd454fff053fb5b93fc1b2ac7";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-X0MW6B53JB"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-X0MW6B53JB');
</script><script>const GLOBAL_CONFIG = {
  linkPageTop: {"enable":true,"title":"同博主们无限进步","addFriendPlaceholder":"昵称（请勿包含博客等字样）：\n网站地址（要求博客地址，请勿提交个人主页）：\n头像图片url（请提供尽可能清晰的图片，我会上传到我自己的图床）：\n描述：\n站点截图（可选）：\n"},
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"一次时空相逢，祝好！","backTitle":"欢迎回来！"},
  LA51: {"enable":true,"ck":"3G4PP4R5LAAt6DBv","LingQueMonitorID":"3GA1sASrkc8hJd7c"},
  greetingBox: {"enable":true,"default":"晚上好👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋, 为下午蓄力充电","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/',
  preloader: undefined,
  friends_vue_info: undefined,
  navMusic: false,
  mainTone: {"mode":"both","api":"https://img2color-go.vercel.app/api?img=","cover_change":true},
  authorStatus: undefined,
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: DiffDay","link":"链接: ","source":"来源: Diffday","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'Diffday',
  title: '大语言模型LLM',
  postAI: '',
  pageFillDescription: '惊喜与惊醒, NLP研究范式转变, 从深度学习到两阶段训练模型, 深度学习期, 两阶段训练大模型, 带来的影响, 中间任务消亡, 技术路线统一, 预训练到通用人工智能, ChatGPT, ChatGPT改变了GPT-3.5什么？, LLM的知识构成, 如何存取知识, LLM的规模效应, 把模型做小会影响LLM的涌现能力么, Transformer的稀疏化, 人机交互, 如何增强LLM的推理能力, 算力约束下的最优培养策略, 为何是OpenAI, Why OpenAI Not Google, Step By Step, 无心插柳, 面对AI恐惧的解答, 成本与挑战, 智算集群成本, 技术挑战, 适合普通玩家的炼丹, 冲击, 奇点临近, 失业预言还是谎言, 代码生成大模型的不足, 正视人性, 成为一个无法被AI取代的人, 拥抱未来, 教育适配, 不要疯魔, 中美AI研究差异, 欧洲在哪?, 一个词话三方, 截止23年6月的评测报告, 综合得分评估, 中文能力点评, 简要评价, 2023年10月OpenAI的价值观变动议论, 瞅瞅MVV概念, 突发事件, 重要参考科技部长的金句踢足球都是盘点射门但是要做到梅西那么好也不容易惊喜与惊醒大语言模型的效果好到令人咋舌我们距离的认知和发展理念距离世界最先进的想法差得有点远出现后年间国内追赶技术很快也提出了一些改进模型分水岭在即为年中体现了应往何处去的发展理念全球看中的人很少梯队明显包括在内对于发展理念的理解都落后一个身位半年到一年的时间国内可能落后年左右鹰酱的风格是进化论模式各个方向上各个公司都搞低成本试探进攻让企业家去承担试错成本进化是允许犯错的甚至是进化不可少的前提最难的事情方向可行性已经被蹚出来了此正是技术最难的一点我方堆资源集中力量办大事的优势可以发挥商业的竞争开源的平替也在出现甚至可能是故意的泄露老美限制我们软件上挡不住那就硬件上挡芯片型号售价美元售价人民币年汇率还有优势供应中国情况缺货断供断供是阉割特供版计算性能相似数据传输速度降低影响集群训练速度和效果还缺货一次只能采购数百片研究范式转变从深度学习到两阶段训练模型深度学习期由大量改进模型及少量改进的模型作为典型的特征抽取器以或叫亦可作为各种具体任务典型的总体技术框架在这些技术下研究目标归纳为如何有效增加模型层深或模型参数容量就是往里不断叠加更深的或层但受限于有限的训练数据总量不够匹配模型容量增加和特征抽取器有限的表达能力不能吸收数据里蕴含的知识最终这条路径相较于飞深度学习方法并没有出现碾压式的优势三元或四元甚至更高阶的模型是不是能覆盖所有语言现象答案是不行上下文之间相关性可能跨度非常大甚至可以从一个段落到另一个段落两阶段训练大模型和模型出现后在学术研究和工业应用角度看都带来了一个技术飞跃子领域的技术方法和框架日趋统一出现一年左右技术栈就基本全线收敛到此二位上图像领域预训练模型应用到下游任务带来的效果收益远不如应用在下游任务那么显著要是蹚通了图像处理的各个子研究领域可能也会逐步消失直接完成终端任务带来的影响中间任务消亡中文分词词性标注命名实体识别句法分析指代消解语义等这类任务不是解决任务的实际需求但作为解决任务的中间阶段或辅助阶段存在而用户其实只关心最终具体任务有没有干好通过大量的预训练已经把这些中间任务作为语言学特征吸收到了参数里无需对中间过程专门建模可端到端直接解决最终任务在技术发展的早期阶段很难一步做好有难度的最终任务科研人员就把难题分而治之技术路线统一最终任务分类文本分类句子相似性计算情感倾向判断意图识别等都是分类任务统一到了为代表的双向语言模型预训练应用的模式聊天机器人翻译文本摘要问答系统等统一到了为代表的自回归语言模型从左到右单向语言模型的模式绝大多数人当时都低估了这条路线的潜力视线中心都聚焦到了模式上预训练到通用人工智能从以后尚在加速演进是触发这次范型转换的关键点在出现前其实出于过渡期最惊艳和最大的贡献是基本实现了让适配人的命令表达方式给出了很好的解决方案增加了易用性和用户体验证明了可以去直接追求理想的模型未来的技术趋势应是越来越大的模型增加预训练数据的多样性预训练模型早期人们普遍更看好一些方式解决下游任务效果占优的领域是因为领域训练数据量大从数据安全角度还没那么快消失但已经不是潮流了随着技术发展目前规模最大的模型几乎清一色类似的模式背后有一定的必然性表现形式可兼容反之则不行分类问题可转换成让生成对应类别字符串的模型形式上就统一了的外在表现形式方式做好任务采取模式数据是海量的要吸收知识需非常多的参数来存储只是必是巨无霸模型模型规模巨大有能力做出及改动这个模型参数的机构必然少就算把模型开源出来中小机构和个人也无力部署更不用说用这种模式去修改模型参数了的模式运行超大模型一定会走向人造通用智能用取代了由此带来新的技术范式转换改变了什么学习资料参数亿学习资料参数亿学习资料参数亿有了海量知识但回答形式和内容却不受约束因为它知道的太多了见到了一个人几辈子都读不完的资料会随意联想像一只脑容量超级大的鹦鹉如何指挥它成了一个目标注入了人类偏好知识什么是好的回答什么是不好的如详细回答是好的带有歧视内容的回答是不好的人类对回答质量好坏的偏好用对话模板去矫正其开卷有益时学到的不规范习惯跟教鹦鹉说话一个道理通过反馈给数据得到一个懂得人话比较礼貌的用人工专门写好的优质对话范例让去接龙的知识构成是足够强大的特征抽取器尚不需做特别的改进那它学到了什么语言类知识和世界知识语言类知识是指语法词性句法语言等有助于人类或机器理解的自然语言知识世界知识指发生在这个世界上的一些真实事件和常识性知识对于类型的语言模型来说只用到亿单词的语料就能学好句法语义等语言学知识事实类知识要更多的训练数据随着模型层深的增加能学到的知识数据以指数级增加把模型看作是以模式参数体现的隐式知识图谱一点也不违和如何存取知识多头注意力占了参数总量的用于计算单词或知识间的相关强度对全局信息进行集成建立知识间的联系大概率不会存储具体的知识点结构占了剩余承担主体知识的存储的输入层其实是某个单词对应的的输出结果将整个句子有关的输入上下文集成到一起的代表整个输入句子的整体信息低层对句子表层模式做出反应高层对语义模式做出反应也就是低层存储语法句法等表层知识中层和高层存储语义及事实概念知识的规模效应目前效果最好的模型参数规模大都超过了千亿如的规模的规模华为盘古模型百度文心随着模型不断增长会发生什么研究证明越大的模型学习效率越高学到了更多知识任务效果更好多数任务都是知识密集型任务近两年都在模型规模增长下获得了极大的效果提升模型规模是解锁新能力的关键出现某种涌现能力带来意想不到的精彩如的推理能力思维链是典型的增强推理能力的技术流利性也是在规模上得以解决的上下文学习里出现的涌现效应等价于隐式的微调但如何有效尚未搞明白想出现涌现能力模型规模大小和具体任务有一定的绑定关系图表第一行的位数加法任务模型只要达到亿参数就可以具备涌现能力但是对倒数第二行的任务而言目前证明只有大小的模型才可以做到这点我们只能说就而言如果模型达到大多数任务可以具备涌现能力把模型做小会影响的涌现能力么年发表的模型这个模型目前做各种任务的效果和大小的基本相当的思路是给更多的数据但是把模型规模做小模型大小只有是的四分之一但是付出的代价是训练数据总量是的四倍所以基本思路是通过放大训练数据量来缩小模型规模在以后训练模型的时候可以考虑先增加训练数据降低模型参数量把模型做小保险起见不应小于先把模型参数利用充分在这个基础上再继续增加数据并推大模型规模第二个小模型代表是发布的开源模型它的做法其实很好理解本质上就是开源的它的思路是完全遵照来做的增加训练数据并把模型规模做小的稀疏化目前规模最大的中相当比例的模型采取了稀疏结构如好处是它可以极大减少的训练时间和在线推理时间有研究表明标准的在训练和推理时它本身也是稀疏激活的既然如此不如直接迁移到稀疏模型随着模型越大稀疏模型带来的收益越明显人机交互从到理解是的早期叫法内涵一致具体做法不同早期实际上就是不知道怎么表达一个任务才对就换不同的单词或句子反复尝试好的任务表达方式这种方式已经被证明是在拟合训练数据的分布做法则是给定命令表达语句试图让理解它尽管表面都是任务的表述但思路是不同的和意思类似就是给几个示例做范本然后让解决新问题也可以理解为某项任务的描述用例子来具象表达任务命令只是是一种更抽象的描述形式用来生成效果很不错在一些任务上超过人类的表现所以也是一个不长久的职位和表面看似都提供了一些例子给但两者有质上的差别拿这些例子当训练数据用反向传播去修正的模型参数但只是拿出例子让看了一眼并没有根据例子去修正参数就要求它去预测新例子正是的神奇之处尚无清晰的原理解释如何增强的推理能力咱们通常不会因为一个人单靠记忆力强就说这个人很聪明还要看他是否有强的推理能力推理能力是智力水平更佳的标准强大推理能力也是让用户认可的心理基础推理能力的本质是综合运用很多知识去推导出新的知识或新结论在推理方面相关的工作和研究可归为大类基于的方法通过合适的提示语或文本更好地激发本身就具有的推理能力在这个方面做了大量很有成效的工作更好的展示出能力的技术方法直接在问题上追加辅助推理在众多领域都很有效第一阶段在提问的问题上追加这句提示语会输出具体的推理过程第二阶段在第一阶段的问题后拼接输出的具体推理过程并再追加如此简单的操作却可以大幅增加在各项推理任务中的效果比如在数学推理测试集上加上提示语后推理准确率直接从原先的提升到了可谓神奇猜测预训练数据里面存在大量的此种数据提示语激发模糊得回忆起某些例子的推导步骤标准的由人工来写推理步骤而大概是通过提示语激活了记忆中某些包含推理步骤的示例人工给出的示例准确性是有保障的所以自然标准效果会更好最早的概念文章发表于年月虽然做法简单但应用后模型推理能力得到了巨大的提升意思是让明白一个道理在推理过程中步子不要迈的太大化大问题为小问题积小胜为大胜提出不久很快在年月一种被称为的改进技术继续助力提升准确率它要求输出多个不同的推理过程和答案然后用投票的方式选出最佳答案将测试集准确率提高到左右简答的方法往往蕴含着深刻的道理虽然起效仍有黑盒的味道应用分治的思想将一个复杂的推理问题分解成若干更易解决的子问题应证了的工作模式要解决问题先把原始问题和交给让给出最终问题的前置子问题然后用原始问题拼接子问题及答案再去问最终问题在预训练过程中引入程序代码和文本一起参与训练这应是实践出来的思路体现出一种通过增强多样性的训练数据来直接增强推理能力的思路为何预训练模型可以从代码中获得额外的推理能力确切原因未知可能开始只是尝试从文本生成代码而代码中往往包含很多文本注释本质上类似于预训练模型做了两种数据的多模态对齐工作支持越来越多的任务类型主要是通过增加预训练数据的多样性来达成算力约束下的最优培养策略假设用于训练的算力总预算如多少天给定是应多增加数据量减少模型参数呢还是说数据量和模型规模同时增加减少训练步数呢选择了同时增加训练数据量和模型参数但采用早停策略来减少训练步数的方案且优先增加模型参数然后才是模型数据量假如算力预算增加了倍那么应增加倍的模型参数量倍的训练数据量此时模型效果最佳单调增加模型参数固定住训练数据量这个做法也是不对的限制了模型的潜力为何是胜在一开始就自我定位比较高要做出人物无关超大型以生成一切的方式解决各种实际问题且应能听懂人类的命令不受外界干扰态度坚定不移比出来更早证明了双向语言模型对于很多任务效果比这种单向语言模型更好尽管如此也没有切换技术路线且开始尝试因效果比差的比较远所以大家都没太当回事甚至不理解它为什么要始终坚持走单向语言模型的路线展示出不错的能力后面技术差距从这里拉开再往后是和的股权设计很特别不受任何股东制约投资者没有控制权协议上是一种债的结构赚完万亿接下来不再盈利了一切回归社会首席科学家生于俄罗斯长大于以色列十多岁岁父母移民到了加拿大从小就一直想搞清楚意识这个东西对一切能助其了解意识的东西感兴趣对其就是一个好的切入点他有一个观点你能高效压缩信息你一定已得到知识这与自己对模型的表述不谋而合在建设引领中他和组织坚信两件事模型架构它要足够深只要有算力只要有数据越大越好在早期用的是后来看到就用改变一切的范式永远有个引擎引擎能不断前进其近期的研究方向是提高模型的可靠性和可控性加快模型从少量数据中学习知识的速度并降低对人工指导的依赖避免出现幻觉两者的技术人员可是差了一个数量级联合创始人面对这个问题如此说道都是站在巨人的肩膀上全行业在计算算法数据上都取得了进步不过在早期做了一些深思熟虑的选择第一个选择就是直面现实很认真的想过如果想要在这个领域取得进展需要做什么也做了很多没用的尝试才看到这些有用的结果还有一点最重要的是让不同团队间可以紧密协作无心插柳过去有人尝试训练一个模型预测亚马逊平台评论的下一个字符最终得到了一个可以分析评论句法的模型但同时也得到了一个达到的情绪分析分类器可以告诉人们这条评论是好评还是差评这个算法现在看来可能不足为奇但在当时他们第一次从底层句法中分析出语义他们就知道必须朝这个方向做下去一些后来看上去很大的成果都不是起初的目标而是在实践过程中潜移默化发展出来的面对恐惧的解答现在有很多对的担忧呼吁暂停更强的研发如此回应到一开始考虑如何构建通用人工智能时也是希望它能造福全人类不存在弄清楚所有安全性后再开始这可能是对的但他不知道该如何执行这一计划唯一可行的是在机器变得完美之前给人们时间来提建议人类开发计算机算法等技术都是并要再推进的每一个阶段去弄清楚如何管理好它们就好像养大一个孩子是大家共同引导给它树立规矩而不是教它毁灭人类成本与挑战当前能做这类事的机构国外不超过家国内不超过家云服务为构建超过枚的计算集群大型商业化后续投入还需更多国内超过枚的企业不超过家高低配合起来只有家有枚的最多只有一家短期内布局的选手十分有限需要长期高成本投入高性能芯片短缺采购成本和运营成本都很高昂挑战的就是资金储备战略意志和实际技术能力含工程能力考虑到成本问题未来或许会出现股份制大模型机构合作共建智算集群成本建设成本以枚价格基准下万枚采购成本亿一台服务器枚才经济那就以一台服务器来核算服务器采购成本通常是数据中心建设成本的那么这个智算集群建设成本通常超过亿训练成本一次完整训练成本超过差不多迭代次完整训练就有亿支出数据采集人工标注等这些软性成本还难以简单计算运营成本网络带宽电力资源人员薪资成本可能也是以亿计的中短期无法盈利用户规模越大亏损可能也会越大得输血支持在年财报上看中云指出亿亿亿百度可能财力上就无法支撑战略意愿上因为与主营收模式冲突也会有持久性的问题假设大厂的资本支出用于投资云基础设施参照技术挑战用参与者的话说预训练一个高精度的千亿模型与训练百亿模型完全不同频繁的随机硬件故障模型梯度爆炸算法中意外的过多内存使用新的和框架中流水线的调试无法从优化器状态中恢复机器间拥塞以及许多许多意外的项目被多次推迟若不幸你们没有足够的训练资源会遭遇到另一个难题我们需要把训练代码适配到不同的硬件平台不同的平台底层算子各不相同很多算子还有所欠缺还有阻碍收敛的各种问题和的计算精度选择问题还有你自己可能犯的各种错误总之看看下面的清单就知道是一个不是东风压倒西风就是西风压倒东风的搏斗过程是对组织能力和资源的一份挑战适合普通玩家的炼丹在资源受限的情况下现在模型亦有点百花齐放的感觉以至于动物名字都有点不够用了普通资源者有哪些值得尝试的开源方向呢将模型规模再扩大扩大把放到甚至再加上目前能收集到最全的再把模型推理方面对资源需求降低些起个名字开源放出来也有意义将这种中文支持不太好的模型加上一个中文数据继续预训练过程很可能会损害基座模型的能力把中文能力做个大幅提升构建一个虽然小但是中文能力相对比较强的大模型也挺有意义做成垂直领域大模型并将其开源在之后或者基础上探索点新的技术改进路线为社区提供些技术启发权威的中文评测集合是另一个维度很有意义的事情冲击社会性拐点已至因为一项大型成本从边际变成固定势必深刻变革各领域奇点临近从能力难度角度从低到高看简单聊天事实性问题写文章写诗简单计算多轮对话复杂指令写代码逻辑推理复杂计算事实一致性当前的表现上的确给力同一个模型完成各种开放任务变成了通用任务助理颠覆人类基本认知高质量对话让人误以为有意识和人格觉醒产生数字生命的感觉模型和数据飞轮转的非常快在很多考试领域已经超越大多数人类人与共存的未来人类一直在畅想机器人三定律年就提出来了人人都配有一个熟读人类知识的王语嫣当前你也可以说她不是真正学会了知识学的是传载知识的语言搭配模式但上下文理解能力和推理能力强要是再配上人形机器人那就不仅仅是个武功军师了以培养学习能力和创造能力为主今后才好在竞争中更显突出越大的机构消耗在语言处理上的成本越高信息协作所以市场非常嗨从音频这种感知智能上升到到认知智能再到更强大的内容生产门槛进一步降低年生产内容可能站到所有的白领工作在一轮生产力变革的前夜知识密集型岗位的生产力变了势必创造新的生产关系关注反应最大的是知识生产知识密集型岗位知识和技能平权进一步前进影响稀缺性互联网民工也有被替代的可能对记忆消耗的解放可以让人们做更多的独立性思考说的具体点可能新闻高等教育图形软件设计等行业的某些工作有被替代的风险金融行业里的许多岗位也会被裁掉大学毕业后花两三年的时间像机器人一样做的工作也是可以让人工智能来但关键的金融和经济决策不会被机器替代失业预言还是谎言核心产业科学教育医疗这是长期最关注的个行业也是整个社会最根本的有些行业的生产资本本质是模型驱动例如医疗就是一个模型行业一个好医生是一个好模型机器人开始抢白领的工作一般来说贩卖焦虑的老套路都是用失业这个绝对痛点戳痛大家脆弱的神经一焦虑你就得乖乖付费总之哪里有焦虑哪里就有生意通过调查显示从教育背景工作经验职业年限和工资数据来看高薪水从业者更容易接触面临影响的风险更大按行业来看信息处理行业受到的影响较大而制造业农业和采矿业则表现出较低的影响风险现在引发的轰动早期的搜索引擎也有过你想想一个搜索框能告诉你所有问题的结果这是一件多么可怕的事情可后来的事情也很清楚论文库各种教程都是大杀器放在封建社会都是要被统治阶级重兵把守的国家机密如今无差别放在大家面前问题是绝大部分人视而不见如果之前那些东西并没有影响大家一个又有什么影响呢大概率一段喧闹后恢复平静就像当初谷歌一样对绝大多数人只是提供了一点方便小部分人觉得捡到了一把机枪变成少数人天天在用的工具绝大多数人非必要不会去碰它社会差距会进一步拉大冲击的也是一小部分人出现的时候很多人惊呼这玩意将改变整个职场江湖谁能想到它只是让大家的工作变得更琐碎了好处是工具的赋能使人站的位置越来越高如果你面对的东西主观性很强客户自己都不知道想要什么或需要大量的想法这种工作短期内还不太行恰好这类工具不但不会取代你且会成为你的帮手凯文凯利说这不是与机器的竞赛如果我们跟它们竞赛我们就会输掉这是一场与机器联合的比赛未来你的薪酬将取决于你和机器人的合作程度代码生成大模型的不足软件开发中实际复杂的部分规划性特别是对于大型软件工程它的架构能力是不足的架构能力实际上是把任务做分解的能力这是一种自顶向下的拆分能力大模型可以在某一个叶子节点或某一块给你平面化的生成代码但纵深的规划能力还需要架构师去补齐另外受限于数其上下文记忆还是不足的如何人机共舞挑拣有效率的事情做是每一位应该去思考的正视人性如历史的教训一书中提到的人生来不自由不平等一些随手可通过搜索引擎查到的东西绝大部分人却在那里疯狂传谣同一个搜索框不同的人查到的东西差距都很大现实世界里的人是没有阅读长文的能力的你再要求他们会使用复杂工具简直是为难大家了太多人在强大工具面前就不知道该如何描述自己想要什么生活就像一个竞技场每个人走到里面惊讶的发现里面摆着一堆武器让大家自己选这些武器从木棍到机枪应有尽有令人不解的是绝大部分人选的是操作简单容易上手的菜刀而不是有一定学习成本的机枪看似公平的竞争最后因为工具的差别变成了单方面的屠杀现实比较复杂一点因为人不止一个工具比如孩子比较蠢选了木棍而他爹有个高达人类社会的大发展回头看也不过百年百年之间人类文明早已经天翻地覆但人类的天性和欲望并没有因此得到任何的进化和改变成为一个无法被取代的人的特点在于它们属于预测型机器如果能为你建模就有对主人你进行替代的危险性了对事物拥有独立见解确实需要付出更多努力不能只能媒体别人怎么说而是要自己主动思考很多价值高的知识不流行有些道理和知识只有少数人知道和学习类比于武林秘籍必然只有少数人拥有市面上充斥着很流行但价值低的书和知识如果一本书很流行但还没被禁说明他有用但没大用或者禁了后放出流行的阉割版本对生活和事物抱有兴趣不断成长不落窠臼增强自己的创造力儿童一般都较有创造力可惜在多数环境中都随着学校的训练消磨而逐渐丧失要允许与众不同且不被嘲笑引导而不是可以创造不失去这份创造力学会忽视他人对你的看法有趣的人之所以有趣是因为他们自己有一套关于成功的定义在射中地方画一个靶心而不是接受别人的成功理念你对自己的投入学习的资源越多就越了解自己自我认知代表着巨大的力量你需要找到最适合自己的位置最适合自己的角色想学焊接就花钱去学想学插花就花钱去学而不是受人意识干扰拥抱未来过去学个知识干一辈子的时代已经渐渐远去了经历了多次科技革命的我们正处在一个加速时期新工具出现越来越快取代效应也越来越快大量受规训的人毕业了被告知还要再学习就情绪上抵触好在社会教做人因为很快意识到市场和工具变化究竟有多快当然也有从事简单重复工作的岗位与再学习逐渐分离但多数也随之甩去了改变生活境遇层次的机会电出来的时候被认为是会带来灾难的巫术无论你是欣喜还是焦虑它终究会在未来的某一天不期而遇市场不会因为禁用而整体不用靠人口和房子的粗旷式发展的大周期已经结束人口下滑也是不可逆的趋势中国正在经历劳动密集向效率提升的转型时代需要新科技新动能来救场人类历史从来不是人和工具之间的搏斗而是人工具替代人的演变当人类整体内大幅增加时个人优势被抹平苦痛会随之重新增加立于潮头意味着更少的竞争与更多的机会保持竞争优势亦不要被欲望收割才能获得轻松幸福的生活教育适配我们小初中训练最多的死记硬背心算重复难度的刷题能力这种反人性的规训是要进行反思的不要成为一个按一定工序墨守成规的执行机器这种能力年后被人工智能淹没是大概率的事情如何思考事物之间的关联而不是只想快点看到老师的总结面向未来学习越是在人工智能时代越是要广泛的跨学科跨领域阅读在人工智能时代能准确描述你要的东西也变得非常有价值美学的认知和表达能力成为一大要素说到底我们是商品社会未来大众会越来越为美的东西买单如果制作过程不再那么重要那么懂美学的孩子就能做出更出色的产品人和计算机是合作者的关系要相信你的创造力例如在一些大的问题的解决上如一部电影的创意能打动人这现在还是大模型所做不到的个性和特长的培养也会显得比以往更为重要一直重要但更为重要了新时代的动手能力就是配合基础学科及美学素养从小锻炼使用现代工具辅助学习的能力对技术的颠覆对艺术的颠覆式必然的正确使用工具对小孩整体帮助是大的这里的正确使用是纯耗时技巧试错的工具化替代是追问反问问答交流而不是简单依赖来提供答案替代自我训练不要疯魔不能因为反感死记硬背就把所有的知识都给刨掉了尽管知识在搜索和大模型里可以查到回答认为大模型工具能代替你提供答案不用学了会使用工具就可以而放弃了刻苦的学习和思考过程那真是走错了路小孩如果不用各种知识来进行学习就像不进行训练一样没有办法在大脑里形成新的神经网络连接是不能凭空创造出创造力的有价值的想象力不是胡思乱想的能力想象力离不开见多识广通过一定量必要的知识学习作为一种预训练的方式是非常重要的读书就是预训练做题就是微调被师傅批评就是校正对齐通过考试和做题使你更容易使用知识人工智能工具的发展是在降低使用的门槛相当于科技平权专家的技能下放给普通人拥有最后比的还是人的创造力和解决问题的能力搜题搜答案的工具一定不要让小孩用此类工具让小孩都是即时满足没有耐心去花时间思考问题怎么解决做题的过程就是花半小时考虑不出来相当于把你大脑里很多知识又重整了一遍中美研究差异美国侧重基础研究中国侧重解决方案其实不仅本世纪所有的科技发展都在太平洋两岸衍生出不同的路径互联网浪潮美国对电商不热衷线上消费渗透率一直上不去中国几乎所有互联网公司都做过电商渗透率冠绝全球规模一度比到加总都高移动互联网中国凭借更好的网络环境更鼓励创新的监管制度直接跳过信用卡时代进入数字支付时代无人驾驶美国侧重车的智能化中国有更好的基建路况网络和交通规划于是选了车路协同的路线产业互联网美国经济产业特点处于微笑曲线的两头科技互联网金融占比高加上人力昂贵企业付费意愿强中国集中在微笑曲线中段作为世界工厂场景丰富产业链完整政策支持高效集中产学研对接十分方便技术验证更好落地这样的大背景导致美国重攻基础研究多是从技术起步中国优势在于场景多需求多往往是场景倒逼技术落地中国民营企业才刚从艰苦奋斗的路上走出来精打细算的习惯改变不了往往从市场需求产品需求开始再慢慢投入科学家和基础研究带动落地美国巨头钱不是问题钱太多才是问题砸钱做基础科学既可以抢占科技高地也需要冲淡垄断者的坏形象美国行业上一个爆款的系列就是先把技术做出来赢围棋冠军但商业落地慢慢探索好几年后这项技术被用于破解蛋白质折叠结构难题参与新药研发才算英雄有用武之地中国用户早期很多用个人电脑自拍头像团队就想做个技术实现头像居中解决这个问题后逐步孵化出人脸检测人像表情智能图等技术用回产品孵化出天天图人像美容技术再用到全民歌这个图像团队就是腾讯优图还有美团的无人机京东的智能供应链都市跟主业投入有关欧洲在哪一句戏谑美国人在创新中国人在应用欧洲人在立法例如大模型商用基本只剩中美两个玩家当然中国当前一些科技领域也走在世界探索的前列相对而言美国还是更强一个词话三方美国虎无知者无畏的自信去行动想到就真敢去做中国卷欧洲守保守高傲和磨叽截止年月的评测报告一份来自清华大学的评测报告综合得分评估总得分生成质量使用与性能安全与合规排名大模型产品加权总得分生成质量使用与性能安全与合规文心一言讯飞星火通义千问天工中文能力点评简要评价大模型产品优势劣势高度灵活高水平的跨领域专业知识中文语义理解能力稍逊色数据时效性弱文心一言语义理解能力和时效性强内容安全把握细微陷阱信息识别能力有待提高知识面覆盖广响应迅速同内容多样性强完整度高响应速度慢时效弱不擅数学问题讯飞星火使用便捷响应速度快内容精炼陷阱信息识别能力和知识专业化水平有待提高通义千问稳定性和准确度高时效性强历史法律数学相关问题表现欠佳天工昆仑万维游戏软件发行商出色的多轮对话能力时效性强响应速度慢数学推理能力较弱年月的价值观变动议论悄然修改了其网站上列出的核心价值观之前的价值观为大胆深思熟虑朴实无华影响力驱动协作和以增长为导向修改后为聚焦通用人工智能坚韧不拔勇往直前坚守规模化效应制造出让人喜爱的东西团队精神大家敏感是因为都能轻易更改那还惘谈核心激起外界对于该公司在既定目标一致性和承诺方面的担忧让人联想起谷歌从其核心价值页面中删除不作恶的时候说明公司行事风格将会与以前不同了还有更一针见血的网友指出貌似经营者不理解价值观使命目标和愿景之间的区别以前的价值观没问题但修改后的价值观不是真正的价值观它们是一些雄心勃勃的陈述的大杂烩当你需要做一些额外的工作来解释这些所谓的价值观就要想想选出的价值观是否堪配其位了映衬出原来的深思熟虑也没做到怪不得被移除瞅瞅概念愿景是目标使命是意义使命和意义也可以合一价值观是准则和文化底线行为准则和信仰使命更抽象宏大愿景更具象可达战略实现愿景愿景支撑使命文化就像空气看不见摸不着但决定生死且会吞噬战略在企业打拼的过程中是创始团队的认知凝结也掺入后来团队共同打磨的认知沉淀是结果由于认同会让团队内部沟通决策的成本大幅降低使命愿景战略公司为什么存在领导者希望公司发展成什么样击败现有及潜在竞争者的计划为组织内所有决策提供前提描述一个持久的事实可是一个无限时期的解答为内部和外部人员提供指导指导战略和组织的发展描述一个鼓舞人心的事实可在一个特定时期内实现主要为内部人员提供指导有些口号也可提供给外部人员列出一系列举措以提供产品或服务创造高于其成本的价值描述公司战略选择的价值方案随市场分析消费者经验试验而不断改善最好严格限制在内部使用当然也有公司每次战略规划或引入外部和尚念经时总动这些概念的主义但恰恰说明其先前沉淀思考的不足反映变化和成长是好的就怕把这个当有魔力的法宝逆风逆水时这些虚的都没用顺水推舟锦上添花还行华为至今为人津津乐道的核心价值观还是以客户为中心一种拉力长期艰苦奋斗一种推力与以奋斗者为本一种动力没怎么变过突发事件的在日被董事会开除了经过一个周末未收回成命被金主爸爸微软收入麾下日原又重回担任微软在董事会获得了一个无投票权的观察员席位能更深入了解内部运作但在重大决策中没有正式投票权这只是治理结构变形的第一步继续拭目以待吧首席科学家已不再在董事会任职在什么狗屁董事会审查工作结论出现之前多方还是就此次宫斗避而不谈还是静待出现吧谷歌开发布会宣传自己吊打竟然用编辑视频劈柴哥在头部中真是一个无能宵小之辈月发布的深陷种族主义和性别主义问题文生图功能刻意拒绝生成白人形象导致功能被迫下架公司创始人布林承认内部测试不充分模型中有团队没完全理解的部分内部代号讽刺金鱼毕竟金鱼以记忆力短著称只是作为扩大训练规模的一个尝试没想到最后训练出来的模型有非常强的记忆力百万上下文窗口两个创始人远离聚光灯在私人岛屿隐居关注绝对隐私一直保持相对公开形象甚至成了派对动物成活方式有着天壤之别劈柴的离职估计不远了本次的发布成了科技界的汪峰被同天的抢尽了风头重要参考张俊林由反思大语言模型的技术精要团队开源的双语预训练模型稀土掘金程序员下岗指南张俊林大语言模型的涌现能力现象与解释张俊林炼制大语言模型的两个现象清华大学大语言模型综合性能评估报告',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-07 15:52:13',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">Diffday</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/archives/"><span> 归档</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/tags/"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本站</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/copyright/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 版权协议</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button only-home" id="travellings_button" title="随机前往一个开往项目网站"><a class="site-page" onclick="anzhiyu.totraveling()" title="随机前往一个开往项目网站" href="javascript:void(0);" rel="external nofollow" data-pjax-state="external"><i class="anzhiyufont anzhiyu-icon-train"></i></a></div><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="http://cdn.diffday.com/picgo/wechatpay.jpg" target="_blank"><img class="post-qr-code-img" alt="微信" src="http://cdn.diffday.com/picgo/wechatpay.jpg"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="http://cdn.diffday.com/picgo/alipay-n.jpeg" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="http://cdn.diffday.com/picgo/alipay-n.jpeg"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">三月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/02/"><span class="card-archive-list-date">二月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">5</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">一月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">十二月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">十月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/08/"><span class="card-archive-list-date">八月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/06/"><span class="card-archive-list-date">六月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item on" id="consoleCommentBarrage" onclick="anzhiyu.switchCommentBarrage()" title="热评开关"><a class="commentBarrage"><i class="anzhiyufont anzhiyu-icon-message"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><!-- if (page.copyright_author && page.copyright_author !== config.author)// a.post-meta-original 转载--><!-- else--><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%BD%91%E6%96%87%E7%B2%BE%E6%91%98/" itemprop="url">网文精摘</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/%E6%8A%80%E6%9C%AF/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>技术</span></a></span></div></div><h1 class="post-title" itemprop="name headline">大语言模型LLM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2023-03-23T09:36:00.000Z" title="发表于 2023-03-23 17:36:00">2023-03-23</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2024-04-07T07:52:13.000Z" title="更新于 2024-04-07 15:52:13">2024-04-07</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">12.4k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>37分钟</span></span><span class="post-meta-separator"></span><span class="post-meta-pv-cv" id="" data-flag-title="大语言模型LLM"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="busuanzi_value_page_pv"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为深圳"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>深圳</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="http://cdn.diffday.com/picgo/20230408222056.png"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://blog.diffday.com/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html"><header><a class="post-meta-categories" href="/categories/%E7%BD%91%E6%96%87%E7%B2%BE%E6%91%98/" itemprop="url">网文精摘</a><a href="/tags/%E6%8A%80%E6%9C%AF/" tabindex="-1" itemprop="url">技术</a><h1 id="CrawlerTitle" itemprop="name headline">大语言模型LLM</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">DiffDay</span><time itemprop="dateCreated datePublished" datetime="2023-03-23T09:36:00.000Z" title="发表于 2023-03-23 17:36:00">2023-03-23</time><time itemprop="dateCreated datePublished" datetime="2024-04-07T07:52:13.000Z" title="更新于 2024-04-07 15:52:13">2024-04-07</time></header><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20230408222056.png" alt="ChatGPT催生AI再一春"></p>
<p>ChatGPT</p>
<blockquote>
<p>GPT:Generative Pre-trained Transformer</p>
<p><font color="blue">科技部长的金句：踢足球都是盘点，射门，但是要做到梅西那么好也不容易</font></p>
</blockquote>
<h1>惊喜与惊醒</h1>
<p>大语言模型的效果好到令人咋舌，我们距离LLM的认知和发展理念，距离世界最先进的想法，差得有点远</p>
<ul>
<li>Bert出现后1~2年间，国内追赶技术很快，也提出了一些改进模型</li>
<li>分水岭在GPT-3，即为2020年中，体现了LLM应往何处去的发展理念，全球看中的人很少，梯队明显
<ul>
<li><font color="red"><u>包括Google在内，对于LLM发展理念的理解，都落后OpenAI一个身位</u></font>（半年到一年的时间）</li>
<li>国内可能落后2年左右</li>
</ul>
</li>
</ul>
<blockquote>
<p>鹰酱的风格是进化论模式，各个方向上各个公司都搞低成本试探进攻，让企业家去承担试错成本。进化是允许犯错的，甚至是进化不可少的前提</p>
</blockquote>
<p><font color="blue"><strong>最难的事情 : 方向可行性</strong>，已经被蹚出来了</font>（此正是技术最难的一点）</p>
<ul>
<li>
<p>我方堆资源，集中力量办大事的优势可以发挥</p>
</li>
<li>
<p>商业的竞争，开源的平替也在出现（甚至可能是故意的泄露）</p>
</li>
<li>
<p>老美限制我们，<font color="red">软件上挡不住，那就硬件上挡</font></p>
<span id="more"></span>
<table>
<thead>
<tr>
<th>芯片型号</th>
<th>售价（美元）</th>
<th>售价（人民币）-- 22年汇率还有优势</th>
<th>供应中国情况</th>
</tr>
</thead>
<tbody>
<tr>
<td>V100</td>
<td>10000</td>
<td>69000</td>
<td></td>
</tr>
<tr>
<td>A800</td>
<td>12000</td>
<td>82800</td>
<td>缺货</td>
</tr>
<tr>
<td>A100</td>
<td>15000</td>
<td>103500</td>
<td><font color="blur">断供</font></td>
</tr>
<tr>
<td>H100</td>
<td>36500</td>
<td>251820</td>
<td><font color="blur">断供</font></td>
</tr>
</tbody>
</table>
</li>
</ul>
<blockquote>
<p>A800是A100阉割特供版，计算性能相似，数据传输速度降低30%，影响AI集群训练速度和效果，还缺货，一次只能采购数百片</p>
</blockquote>
<h1>NLP研究范式转变</h1>
<h2 id="从深度学习到两阶段训练模型">从深度学习到两阶段训练模型</h2>
<h3 id="深度学习期">深度学习期</h3>
<ul>
<li>
<p>由大量改进LSTM模型及少量改进的CNN模型作为典型的特征抽取器</p>
</li>
<li>
<p>以<font color="blue">Sequence to Sequence（或叫encoder-decoder亦可）+Attention</font>作为各种具体任务典型的总体技术框架</p>
</li>
</ul>
<p>在这些技术下，研究目标归纳为<u>如何有效增加模型层深或模型参数容量</u>。就是往encoder-decoder里不断叠加更深的LSTM或CNN层。</p>
<p>但<font color="blue">受限于有限的训练数据总量（不够匹配模型容量增加）</font>和特征抽取器有限的表达能力（不能吸收数据里蕴含的知识），最终这条路径相较于飞深度学习方法并没有出现碾压式的优势</p>
<blockquote>
<p>三元或四元甚至更高阶的模型是不是能覆盖所有语言现象。答案是不行</p>
<p>上下文之间相关性可能跨度非常大，甚至可以从一个段落到另一个段落</p>
</blockquote>
<h3 id="两阶段训练大模型">两阶段训练大模型</h3>
<p>Bert和GPT模型出现后，在学术研究和工业应用角度看，都带来了一个技术飞跃，子领域的技术方法和框架日趋统一</p>
<blockquote>
<p>Bert出现一年左右，技术栈就基本全线收敛到此二位上。</p>
<p>图像领域预训练模型（vision transformer）应用到下游任务，带来的效果收益，远不如Bert/GPT应用在NLP下游任务那么显著，要是蹚通了，图像处理的各个子研究领域可能也会逐步消失，直接完成终端任务</p>
</blockquote>
<h3 id="带来的影响">带来的影响</h3>
<h4 id="中间任务消亡">中间任务消亡</h4>
<p>中文分词，词性标注，命名实体识别（NER），句法分析，指代消解，语义Parser等，这类任务不是解决任务的实际需求，但作为解决任务的中间阶段或辅助阶段存在。而用户其实只关心最终具体任务有没有干好。</p>
<p>通过大量的预训练，Bert/GPT已经把这些中间任务作为语言学特征，吸收到了Transformer参数里，无需对中间过程专门建模，可端到端直接解决最终任务。</p>
<blockquote>
<p>在技术发展的早期阶段，很难一步做好有难度的最终任务，科研人员就把难题分而治之</p>
</blockquote>
<h4 id="技术路线统一">技术路线统一</h4>
<p>最终任务分类：NLU+NLG</p>
<p>NLU：文本分类，句子相似性计算，情感倾向判断，意图识别等，都是分类任务。</p>
<blockquote>
<p>统一到了Bert为代表的“双向语言模型预训练”+应用fine-tuning的模式</p>
</blockquote>
<p>NLG：聊天机器人，翻译，文本摘要，问答系统等</p>
<blockquote>
<p>统一到了GPT-2为代表的“自回归语言模型（从左到右单向语言模型）+zero/few shot prompt”的模式</p>
</blockquote>
<p>绝大多数人当时都低估了GPT这条路线的潜力，视线中心都聚焦到了Bert模式上。</p>
<h2 id="预训练到通用人工智能">预训练到通用人工智能</h2>
<blockquote>
<p>从GPT-3以后，尚在加速演进</p>
</blockquote>
<h3 id="ChatGPT">ChatGPT</h3>
<p>ChatGPT是触发这次范型转换的关键点，在InstructGPT出现前，LLM其实出于过渡期。</p>
<blockquote>
<p>ChatGPT最惊艳和最大的贡献是基本<font color="blue">实现了让LLM适配人的命令表达方式，给出了很好的解决方案，增加了易用性和用户体验</font></p>
<p><font color="blue">证明了可以去直接追求理想的LLM模型，未来的技术趋势应是越来越大的LLM模型，增加预训练数据的多样性</font></p>
</blockquote>
<ul>
<li>
<p>预训练模型早期，人们普遍更看好Bert一些</p>
<ul>
<li>fine-tuning方式解决下游任务，Bert&gt;GPT</li>
<li><font color="red"><u>Fine-tuning效果占优的领域是因为领域训练数据量大，从数据安全角度，fine-tuning还没那么快消失，但已经不是潮流了</u></font></li>
</ul>
</li>
<li>
<p>随着技术发展，目前规模最大的LLM模型，几乎清一色类似GPT-3的模式，背后有一定的必然性</p>
<ul>
<li>
<p><font color="blue">NLG表现形式可兼容NLU</font>，反之则不行。分类问题可转换成让LLM生成对应类别字符串，Google的T5模型，形式上就统一了NLU+NLG的外在表现形式。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20230322170650.png" style="zoom:50%;" />
</li>
<li>
<p>Zero/few shot promot方式做好任务，采取GPT模式</p>
<ul>
<li>数据是海量的，要吸收知识，需非常多的参数来存储只是，必是巨无霸模型</li>
<li>模型规模巨大，有能力做出及改动这个模型参数的机构必然少</li>
<li><font color="red">就算把模型开源出来，中小机构和个人也无力部署，更不用说用fine-tuning这种模式去修改模型参数了</font></li>
<li><font color="blue">LLM as Service的模式运行</font>，超大模型一定会走向AGI（人造通用智能）</li>
<li>ChatGPT用Instruct取代了prompting，由此带来新的技术范式转换</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="ChatGPT改变了GPT-3-5什么？">ChatGPT改变了GPT-3.5什么？</h4>
<blockquote>
<p>GPT-1学习资料5G，参数1.17亿</p>
<p>GPT-2学习资料40G，参数15亿</p>
<p>GPT-3学习资料45T，参数1750亿</p>
</blockquote>
<p>GPT有了海量知识，但回答形式和内容却不受约束，因为它知道的太多了。见到了一个人几辈子都读不完的资料，会随意联想，像一只脑容量超级大的鹦鹉，如何指挥它成了一个目标。</p>
<p>ChatGPT注入了人类偏好知识，什么是好的回答，什么是不好的。如详细回答是好的，带有歧视内容的回答是不好的，人类对回答质量好坏的偏好，用对话模板去矫正其开卷有益时学到的不规范习惯（跟教鹦鹉说话一个道理），通过reward-model反馈给LLM数据，得到一个懂得人话，比较礼貌的LLM。</p>
<blockquote>
<p>用人工专门写好的优质对话范例让GPT去接龙</p>
</blockquote>
<h3 id="LLM的知识构成">LLM的知识构成</h3>
<blockquote>
<p>Transformer是足够强大的特征抽取器，尚不需做特别的改进，那它学到了什么？</p>
</blockquote>
<p>语言类知识和世界知识</p>
<ul>
<li>语言类知识是指语法，词性，句法，语言等有助于人类或机器理解的自然语言知识</li>
<li>世界知识指发生在这个世界上的一些真实事件和常识性知识</li>
</ul>
<p>对于Bert类型的语言模型来说，只<u>用1000w到1亿单词的语料，就能学好句法语义等语言学知识</u>；<font color="blue">事实类知识要更多的训练数据</font>。</p>
<p>随着Transformer模型层深的增加，能学到的知识数据以指数级增加，把<font color="red">模型看作是以模式参数体现的隐式知识图谱</font>，一点也不违和。</p>
<h4 id="如何存取知识">如何存取知识</h4>
<ul>
<li>多头注意力（MHA）占了参数总量的1/3，用于计算单词或知识间的相关强度，对全局信息进行集成，建立知识间的联系，大概率不会存储具体的知识点</li>
<li>FFN（Feed Forward Network）结构占了剩余2/3，承担主体知识的存储。FFN的输入层其实是某个单词对应的MHA的输出结果Embedding，将整个句子有关的输入上下文集成到一起的Embedding，代表整个输入句子的整体信息</li>
<li>Transformer低层对句子表层模式做出反应，高层对语义模式做出反应。也就是<font color="blue">低层FFN存储语法，句法等表层知识；中层和高层存储语义及事实概念知识</font></li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20230322170801.png" alt=""></p>
<h3 id="LLM的规模效应">LLM的规模效应</h3>
<p>目前效果最好的LLM模型，参数规模大都超过了千亿（100B），如OpenAI的GPT-3规模175B，Google的LaMDA规模540B，华为盘古模型200B，百度文心260B，随着模型不断增长，会发生什么？</p>
<ul>
<li>
<p>研究证明，越大的LLM模型学习效率越高，学到了更多知识，任务效果更好。多数NLU任务，都是知识密集型任务，近两年都在模型规模增长下获得了极大的效果提升。</p>
</li>
<li>
<p>模型规模是解锁LLM新能力的关键，出现某种涌现能力带来意想不到的精彩，如chatGPT的推理能力。</p>
<blockquote>
<p>思维链是典型的增强LLM推理能力的技术，流利性也是在规模上得以解决的。</p>
<p><font color="red">上下文学习里出现的涌现效应，等价于隐式的微调</font>，但如何有效尚未搞明白</p>
</blockquote>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20230816104609.png" alt="In Context Learning 的涌现能力和模型规模的关系"></p>
<blockquote>
<p><font color="dodgerblue">想出现涌现能力，模型规模大小和具体任务有一定的绑定关系</font></p>
</blockquote>
<p>图表第一行的3位数加法任务，模型只要达到 13B（130亿参数），就可以具备涌现能力，但是对倒数第二行的 Word in Context Benchmark任务而言，目前证明，只有540B 大小的模型才可以做到这点。我们只能说，就In Context Learning而言，如果模型达到 100B， 大多数任务可以具备涌现能力。</p>
<h4 id="把模型做小会影响LLM的涌现能力么">把模型做小会影响LLM的涌现能力么</h4>
<p>DeepMind 2021年发表的模型 Chinchilla，这个模型目前做各种任务的效果，和 540B大小的PaLM 基本相当。Chinchilla的思路是给更多的数据，但是把模型规模做小<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="张俊林 [大语言模型的涌现能力——现象与解释](https://www.jiqizhixin.com/articles/2023-04-28-4)
">[4]</span></a></sup>。</p>
<p>Chinchilla模型大小只有 70B，是Gopher的四分之一，但是<font color="dodgerblue">付出的代价是训练数据总量，是Gopher的四倍，所以基本思路是通过放大训练数据量，来缩小模型规模</font>。</p>
<p>在以后训练模型的时候，可以考虑 <u>先增加训练数据</u> ，降低模型参数量，把模型做小(保险起见，不应小于70B)，先把模型参数利用充分，在这个基础上，再继续增加数据，并推大模型规模。</p>
<p>第二个小模型代表是 Meta 发布的开源模型 LLaMA，它的做法其实很好理解，本质上就是开源的 Chinchilla，它的思路是完全遵照 Chinchilla 来做的，增加训练数据，并把模型规模做小</p>
<h3 id="Transformer的稀疏化">Transformer的稀疏化</h3>
<p>目前规模最大的LLM中，相当比例的模型采取了稀疏结构，如GPT-3，PaLM，好处是它可以极大减少LLM的训练时间和在线推理时间</p>
<p>有研究表明，标准的Dense Transformer在训练和推理时，它本身也是稀疏激活的，既然如此，不如直接迁移到稀疏模型</p>
<p>随着模型越大，稀疏模型带来的收益越明显</p>
<h3 id="人机交互">人机交互</h3>
<blockquote>
<p>从In Context Learning到Instruct理解</p>
</blockquote>
<p>Zero shot prompt是Instruct的早期叫法，内涵一致，具体做法不同</p>
<ul>
<li>早期Zero shot prompt实际上就是不知道怎么表达一个任务才对，就换不同的单词或句子，反复尝试好的任务表达方式。这种方式已经被证明是在拟合训练数据的分布</li>
<li>Instruct做法则是给定命令表达语句，试图让LLM理解它，尽管表面都是任务的表述，但思路是不同的</li>
</ul>
<p>In Context Learning和Few shot prompt意思类似，就是<font color="red">给LLM几个示例做范本，然后让LLM解决新问题</font></p>
<ul>
<li>In Context Learning也可以理解为某项任务的描述（用例子来具象表达任务命令），<font color="dodgerblue">只是Instruct是一种更抽象的描述形式</font></li>
</ul>
<blockquote>
<p>LLM用来生成Instruct效果很不错，在一些任务上超过人类的表现，所以Prompt engineer也是一个不长久的职位</p>
</blockquote>
<ul>
<li><font color="dodgerblue">Fine-tuning和In Context Learning表面看似都提供了一些例子给LLM，但两者有质上的差别</font>
<ul>
<li>Fine-tuning拿这些例子当训练数据，用反向传播去修正LLM的模型参数</li>
<li>但In Context Learning只是拿出例子让LLM看了一眼，并没有根据例子去修正参数，就要求它去预测新例子（正是In Context Learning的神奇之处，尚无清晰的原理解释）</li>
</ul>
</li>
</ul>
<h3 id="如何增强LLM的推理能力">如何增强LLM的推理能力</h3>
<p>咱们通常不会因为一个人单靠记忆力强，就说这个人很聪明，还要看他是否有强的推理能力，推理能力是智力水平更佳的标准。强大推理能力也是让用户认可LLM的心理基础。</p>
<blockquote>
<p>推理能力的本质是综合运用很多知识，去推导出新的知识或新结论</p>
</blockquote>
<p>在LLM推理方面相关的工作和研究，可归为4大类</p>
<ul>
<li>
<p>基于Prompt的方法，通过合适的提示语或文本，更好地激发LLM本身就具有的推理能力，google在这个方面做了大量很有成效的工作</p>
<ul>
<li>更好的展示出能力的技术方法，直接在问题上追加辅助推理Prompt，在众多领域都很有效
<ul>
<li>第一阶段在提问的问题上追加“Let’s think step by step”这句提示语，LLM会输出具体的推理过程；第二阶段，在第一阶段的问题后，拼接LLM输出的具体推理过程，并再追加Prompt。如此简单的操作，却可以大幅增加LLM在各项推理任务中的效果，比如在数学推理测试集GSM8K上，加上提示语后，推理准确率直接从原先的10.4%提升到了40.4%，可谓神奇。（猜测预训练数据里面存在大量的此种数据，提示语激发LLM模糊得“回忆”起某些例子的推导步骤）</li>
</ul>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20230322171145.png" alt=""></p>
</li>
<li>
<p>COT</p>
<p>标准的COT由人工来写推理步骤，而Zero-shot COT大概是通过提示语，激活了记忆中某些包含推理步骤的示例。人工给出的示例，准确性是有保障的，所以自然标准CoT效果会更好。</p>
<blockquote>
<p>最早的COT概念文章发表于22年1月，虽然做法简单，但应用COT后模型推理能力得到了巨大的提升</p>
</blockquote>
<p>意思是<font color="blue">让LLM明白一个道理：在推理过程中，步子不要迈的太大，化大问题为小问题，积小胜为大胜</font></p>
<p>COT提出不久，很快在22年3月，一种被称为“Self-Consistency”的改进技术继续助力提升准确率，它要求LLM输出多个不同的推理过程和答案，然后用投票的方式选出最佳答案，将GSM8K测试集准确率提高到83%左右。简答的方法往往蕴含着深刻的道理。虽然COT起效仍有黑盒的味道。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20230322172430.png" alt=""></p>
</li>
<li>
<p>Least-to-most prompting，应用分治的思想，将一个复杂的推理问题，分解成若干更易解决的子问题，应证了COT的工作模式。<font color="blue">要解决Final Q问题，先把原始问题和Prompt交给LLM，让LLM给出最终问题的前置子问题sub Q，然后用原始问题拼接子问题sub Q及答案，再去问LLM最终问题Final Q</font></p>
</li>
<li>
<p>在预训练过程中<u><strong>引入程序代码，和文本一起参与训练</strong></u>，这应是OpenAI实践出来的思路</p>
<ul>
<li>体现出一种通过增强多样性的训练数据，来直接增强推理能力的思路</li>
<li>为何预训练模型可以从代码中获得额外的推理能力，确切原因未知。<font color="red">可能开始只是尝试从文本生成代码，而代码中往往包含很多文本注释，本质上类似于预训练模型做了两种数据的多模态对齐工作</font></li>
<li>支持越来越多的任务类型，主要是通过增加LLM预训练数据的多样性来达成</li>
</ul>
</li>
</ul>
<h1>算力约束下的最优培养策略</h1>
<p>假设用于训练LLM的算力总预算（如多少GPU天）给定，是应多增加数据量，减少模型参数呢，还是说数据量和模型规模同时增加，减少训练步数呢？</p>
<p><font color="blue"> OpenAI选择了同时增加训练数据量和模型参数，但采用早停策略来减少训练步数的方案</font></p>
<ul>
<li><font color="red">且优先增加模型参数，然后才是模型数据量</font></li>
<li>假如算力预算增加了10倍，那么应增加5.5倍的模型参数量，1.8倍的训练数据量，此时模型效果最佳</li>
<li>单调增加模型参数，固定住训练数据量，这个做法也是不对的，限制了模型的潜力</li>
</ul>
<h1>为何是OpenAI</h1>
<p>胜在一开始就自我定位比较高，要做出人物无关超大型LLM，以生成一切的方式解决各种实际问题，且应能听懂人类的命令。不受外界干扰态度坚定不移。</p>
<ul>
<li>GPT-1比Bert出来更早，Bert证明了双向语言模型对于很多NLU任务，效果比GPT这种单向语言模型更好，尽管如此，GPT-2也没有切换技术路线，且开始尝试zero/few shot prompt。因效果比Bert+fine-tuning差的比较远，所以大家都没太当回事，甚至不理解它为什么要始终坚持走单向语言模型的路线。</li>
<li>GPT-3展示出不错的zero/few shot prompt能力，后面技术差距从这里拉开，再往后是InstructGPT和ChatGPT</li>
</ul>
<blockquote>
<p>OpenAI的股权设计很特别，不受任何股东制约，投资者没有控制权，协议上是一种债的结构，赚完2万亿，接下来不再盈利了，一切回归社会</p>
</blockquote>
<p>OpenAI首席科学家Ilya Sutskever生于俄罗斯，长大于以色列，十多岁岁父母移民到了加拿大。从小就一直想搞清楚意识（consciousness）这个东西，对一切能助其了解意识的东西感兴趣，AI对其就是一个好的切入点，他有一个观点，你能高效压缩信息，你一定已得到知识（这与自己对模型的表述不谋而合）。在OpenAI建设引领中他和组织坚信两件事</p>
<ol>
<li>模型架构，它要足够深，bigness is betterness，只要有算力，只要有数据，越大越好。在OpenAI早期用的是LSTM，后来看到Transformer就用Transformer</li>
<li>改变一切的范式永远有个引擎，引擎能不断前进</li>
</ol>
<p>其近期的研究方向是提高模型的可靠性和可控性，加快模型从少量数据中学习知识的速度，并降低对人工指导的依赖，避免出现幻觉</p>
<h2 id="Why-OpenAI-Not-Google">Why OpenAI Not Google</h2>
<p>两者的技术人员可是差了一个数量级，OpenAI联合创始人Greg Brockman面对这个问题如此说道：</p>
<p>都是站在巨人的肩膀上，全AI行业在计算，算法，数据上都取得了进步，不过OpenAI在早期，做了一些深思熟虑的选择。</p>
<ul>
<li>
<p>第一个选择就是直面现实，很认真的想过如果想要在这个领域取得进展，需要做什么，也做了很多没用的尝试，才看到这些有用的结果</p>
</li>
<li>
<p>还有一点最重要的是让不同团队间可以紧密协作</p>
</li>
</ul>
<h2 id="Step-By-Step">Step By Step</h2>
<h3 id="无心插柳">无心插柳</h3>
<p>OpenAI过去有人尝试训练一个模型，预测亚马逊平台评论的下一个字符。最终得到了一个可以分析评论句法的模型，但同时也得到了一个达到SOTA的情绪分析分类器，可以告诉人们这条评论是好评还是差评。这个算法现在看来可能不足为奇，但在当时他们第一次从底层句法中分析出语义，他们就知道必须朝这个方向做下去。</p>
<blockquote>
<p>一些后来看上去很大的成果，都不是起初的目标，而是在实践过程中潜移默化发展出来的。</p>
</blockquote>
<h3 id="面对AI恐惧的解答">面对AI恐惧的解答</h3>
<p>现在有很多对AI的担忧，呼吁暂停更强AI的研发。Brockman如此回应到：</p>
<ul>
<li>一开始考虑如何构建通用人工智能时，也是希望它能造福全人类。</li>
<li>不存在弄清楚所有安全性后再开始，这可能是对的，但他不知道该如何执行这一计划</li>
<li>唯一可行的是在机器变得完美之前，给人们时间来提建议。</li>
</ul>
<p>人类开发计算机，算法等技术，都是step by step，并要再推进的每一个阶段去弄清楚如何管理好它们，就好像养大一个孩子，是大家共同引导，给它树立规矩，而不是教它毁灭人类。</p>
<h1>成本与挑战</h1>
<p>当前能做ChatGPT这类事的机构，国外不超过5家，国内不超过3家</p>
<ul>
<li>Azure云服务为ChatGPT构建超过1w枚A100/H100的计算集群，大型商业化后续投入还需更多</li>
<li>国内超过w枚GPU的企业不超过5家（高低配合起来）只有1家，有w枚A100的最多只有一家，短期内布局的选手十分有限。需要长期高成本投入，高性能GPU芯片短缺，采购成本和运营成本都很高昂，挑战的就是资金储备，战略意志和实际技术能力（含工程能力）。</li>
<li>考虑到成本问题，未来或许会出现股份制大模型，机构合作共建</li>
</ul>
<h2 id="智算集群成本">智算集群成本</h2>
<ul>
<li>建设成本
<ul>
<li>以A800 10w/枚价格基准下，万枚采购成本10亿</li>
<li>一台服务器4-8枚GPU才经济，那就以40w一台GPU服务器来核算</li>
<li><font color="red">服务器采购成本通常是数据中心建设成本的30%</font>，那么这个智算集群建设成本通常超过30亿</li>
</ul>
</li>
<li>训练成本
<ul>
<li>ChatGPT一次完整训练成本超过$1200w，差不多￥8000w，迭代10次完整训练，就有8亿支出</li>
<li>数据采集，人工标注等这些软性成本还难以简单计算</li>
</ul>
</li>
<li>运营成本
<ul>
<li>网络带宽，电力资源，人员薪资，成本可能也是以亿计的</li>
</ul>
</li>
</ul>
<p>中短期无法盈利，用户规模越大，亏损可能也会越大，得输血支持。在22年财报上看，BAT中云指出56亿，266亿，311亿。百度可能财力上就无法支撑，战略意愿上因为与主营收模式冲突也会有持久性的问题。</p>
<blockquote>
<p>假设大厂50%的资本支出用于投资云基础设施（参照Amazon）</p>
</blockquote>
<h2 id="技术挑战">技术挑战</h2>
<p>用GLM-130B参与者的话说，“预训练一个高精度的千亿模型与训练百亿模型完全不同”<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="GLM团队.[GLM-130B：开源的双语预训练模型](https://keg.cs.tsinghua.edu.cn/glm-130b/zh/posts/glm-130b/)
">[2]</span></a></sup>：</p>
<p>频繁的随机硬件故障、模型梯度爆炸、算法中意外的过多内存使用、新的 Megatron 和 DeepSpeed 框架中 3D 流水线的调试、无法从优化器状态中恢复、机器间 TCP 拥塞，以及许多许多意外的 “bug”，项目被多次推迟。</p>
<p>若不幸你们没有足够的训练资源，会遭遇到另一个难题：我们需要把训练代码适配到不同的硬件平台。不同的平台底层算子各不相同，很多算子还有所欠缺，还有阻碍收敛的各种问题，Softmax 和 Attention 的计算精度选择问题，还有你自己可能犯的各种错误，总之看看下面的清单，就知道是一个不是东风压倒西风，就是西风压倒东风的搏斗过程，是对组织能力和资源的一份挑战。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20230516171310.png" style="zoom:40%;" alt="GLM-130B训练解决的技术问题"/>
<h2 id="适合普通玩家的炼丹">适合普通玩家的炼丹</h2>
<p>在资源受限的情况下，现在模型亦有点百花齐放的感觉，以至于动物名字都有点不够用了。普通资源者有哪些值得尝试的开源方向呢？<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="张俊林 [炼制“大语言模型”的两个现象](https://k.sina.cn/article_2674405451_9f68304b019013nky.html)
">[5]</span></a></sup></p>
<ul>
<li>将模型规模再扩大扩大，把 LLaMA 放到 30B 甚至 65B，再加上目前能收集到最全的 instruct，再把<font color="dodgerblue">模型推理方面对资源需求降低些</font>，起个名字，开源放出来，也有意义</li>
<li>将LLaMA 这种中文支持不太好的模型，加上一个中文数据继续预训练过程（很可能会损害基座模型的能力），把中文能力做个大幅提升，构建一个虽然小，但是中文能力相对比较强的大模型，也挺有意义</li>
<li>做成垂直领域大模型，并将其开源</li>
<li>在 LLaMA+instruct 之后，或者 ChatGLM 基础上，探索点新的技术改进路线，为 LLM 社区提供些技术启发</li>
<li>权威的中文 LLM 评测集合，是另一个维度很有意义的事情</li>
</ul>
<h1>冲击</h1>
<p><font color="blur">社会性拐点已至，因为一项大型成本从边际变成固定，势必深刻变革各领域。</font></p>
<h2 id="奇点临近">奇点临近</h2>
<p>从AI能力难度角度从低到高看</p>
<ol>
<li>简单聊天，事实性问题，写文章，写诗</li>
<li>简单计算，多轮对话</li>
<li>复杂指令，写代码</li>
<li>逻辑推理，复杂计算，事实一致性</li>
</ol>
<p>当前ChatGPT的表现上的确给力</p>
<ul>
<li>同一个模型完成各种开放任务，变成了通用任务助理，颠覆人类基本认知
<ul>
<li>高质量对话让人误以为AI有意识和人格觉醒，产生数字生命的感觉
<ul>
<li>模型和数据飞轮转的非常快，在很多考试领域已经超越大多数人类</li>
<li>人与AI共存的未来人类一直在畅想，机器人三定律1953年就提出来了</li>
</ul>
</li>
<li><font color="blue">人人都配有一个熟读人类知识的王语嫣</font>，当前你也可以说她不是真正学会了知识，学的是传载知识的语言搭配模式，但上下文理解能力和推理能力强，要是再配上人形机器人，那就不仅仅是个武功军师了。</li>
<li>以培养学习能力和创造能力为主，今后才好在竞争中更显突出。</li>
</ul>
</li>
<li>越大的机构，消耗在语言处理上的成本越高（信息协作），所以市场非常嗨
<ul>
<li>从cv，音频这种感知智能上升到NLP到认知智能，再到更强大的AIGC。PGC -&gt; PGC+UGC -&gt; AIGC，内容生产门槛进一步降低，2025年AI生产内容可能站到所有的10%</li>
<li>白领工作在一轮生产力变革的前夜，知识密集型岗位的生产力变了，势必创造新的生产关系。
<ul>
<li>关注/反应最大的是知识生产/知识密集型岗位，知识和技能平权进一步前进，影响稀缺性，互联网民工也有被替代的可能<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="稀土掘金.[Github Copilot 程序员下岗指南](https://juejin.cn/post/7218014583863656503)
">[3]</span></a></sup></li>
<li>对记忆消耗的解放，可以让人们做更多的独立性思考</li>
<li>说的具体点，可能新闻，高等教育，图形，软件设计等行业的某些工作，有被AI替代的风险。金融行业里的许多岗位也会被裁掉，大学毕业后花两三年的时间像机器人一样做excel的工作，也是可以让人工智能来。但关键的金融和经济决策不会被机器替代</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="失业，预言还是谎言">失业，预言还是谎言</h2>
<ul>
<li>
<p><font color="blur"> 核心产业：科学，教育，医疗，这是OpenAI长期最关注的3个行业，也是整个社会最根本的。</font>（有些行业的生产资本本质是模型驱动，例如医疗就是一个模型行业，一个好医生是一个好模型）</p>
</li>
<li>
<p>机器人开始抢白领的工作，一般来说<u>贩卖焦虑的老套路都是用失业这个绝对痛点</u>，戳痛大家脆弱的神经，<font color="red">一焦虑你就得乖乖付费</font>。总之哪里有焦虑，哪里就有生意。</p>
</li>
</ul>
<p>通过调查显示，从教育背景，工作经验，职业年限和工资数据来看，高薪水从业者更容易接触LLM，面临影响的风险更大。按行业来看，信息处理行业受到的影响较大，而制造业，农业和采矿业则表现出较低的影响风险。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20240119154137.png" alt="卢德分子其实是熟练工人，而不是形象中的蠢货" style="zoom:60%;" />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20240119154459.png" alt="新世界更贴近科幻片里描绘的二元世界" style="zoom:60%;" />
<p>现在ChatGPT引发的轰动，早期的搜索引擎也有过，你想想一个搜索框能告诉你所有问题的结果，这是一件多么可怕的事情，可后来的事情也很清楚。</p>
<blockquote>
<p><u><strong>论文库，各种教程，都是大杀器，放在封建社会都是要被统治阶级重兵把守的国家机密</strong></u>，如今无差别放在大家面前，<u><strong>问题是绝大部分人视而不见</strong></u>，如果之前那些东西并没有影响大家，一个chatgpt又有什么影响呢？</p>
</blockquote>
<ul>
<li>大概率一段喧闹后恢复平静，就像当初谷歌一样，<font color="blue">对绝大多数人只是提供了一点方便，小部分人觉得捡到了一把机枪 (变成少数人天天在用的工具，绝大多数人非必要不会去碰它)，社会差距会进一步拉大</font>，冲击的也是一小部分人</li>
<li>Excel出现的时候，很多人惊呼这玩意将改变整个职场江湖，谁能想到，它只是让大家的工作变得更琐碎了。</li>
<li>好处是工具的赋能，使人站的位置越来越高</li>
</ul>
<p>如果你面对的东西主观性很强，客户自己都不知道想要什么，或需要大量的想法，这种工作短期内AI还不太行，恰好这类工具不但不会取代你，且会成为你的帮手。</p>
<blockquote>
<p>凯文-凯利说：这不是与机器的竞赛，如果我们跟它们竞赛，我们就会输掉。这是一场与机器联合的比赛，未来你的薪酬将取决于你和机器人的合作程度。</p>
</blockquote>
<h3 id="代码生成大模型的不足">代码生成大模型的不足</h3>
<p>软件开发中实际复杂的部分，规划性，特别是对于大型软件工程，它的架构能力是不足的。架构能力实际上是把任务做分解的能力，这是一种自顶向下的拆分能力。大模型可以在某一个叶子节点或某一块给你平面化的生成代码，但纵深的规划能力还需要架构师去补齐。</p>
<p>另外，受限于Token数，其上下文记忆还是不足的，如何人机共舞挑拣有效率的事情做，是每一位应该去思考的。</p>
<h3 id="正视人性">正视人性</h3>
<blockquote>
<p>如<a href="https://blog.diffday.com/%E5%8E%86%E5%8F%B2%E7%9A%84%E6%95%99%E8%AE%AD.html">《历史的教训》</a>一书中提到的，人生来不自由不平等</p>
</blockquote>
<p>一些随手可通过搜索引擎查到的东西，绝大部分人却在那里疯狂传谣。同一个搜索框，不同的人查到的东西，差距都很大。现实世界里，80%的人是没有阅读长文的能力的，你再要求他们会使用复杂工具简直是为难大家了，太多人在强大工具面前就不知道该如何描述自己想要什么。</p>
<p><font color="purple">生活就像一个竞技场，每个人走到里面惊讶的发现里面摆着一堆武器，让大家自己选。这些武器从木棍到机枪应有尽有，令人不解的是，绝大部分人选的是操作简单容易上手的菜刀，而不是有一定学习成本的机枪。看似公平的竞争，最后因为工具的差别，变成了单方面的屠杀</font>。</p>
<blockquote>
<p>现实比较复杂一点，因为人不止一个工具，比如孩子比较蠢，选了木棍，而他爹有个高达。</p>
</blockquote>
<p>人类社会的大发展，回头看也不过百年，百年之间，人类文明早已经天翻地覆，但人类的天性和欲望并没有因此得到任何的进化和改变。</p>
<h3 id="成为一个无法被AI取代的人">成为一个无法被AI取代的人</h3>
<p>AI的特点在于它们属于预测型机器，如果能为你建模，就有对主人（你）进行替代的危险性了</p>
<ul>
<li>
<p>对事物拥有独立见解确实需要付出更多努力</p>
<ul>
<li>不能只能媒体/别人怎么说，而是要自己主动思考
<ul>
<li>很多价值高的知识不流行（有些道理和知识只有少数人知道和学习，类比于武林秘籍，必然只有少数人拥有）</li>
<li>市面上充斥着很流行但价值低的书和知识（如果一本书很流行，但还没被禁，说明他有用，但没大用；或者禁了后放出流行的阉割版本）</li>
</ul>
</li>
<li>对生活和事物抱有兴趣，不断成长，不落窠臼[$k\bar{e}\ ji\grave{u}$]</li>
</ul>
</li>
<li>
<p>增强自己的创造力</p>
<ul>
<li>儿童一般都较有创造力，可惜在多数环境中都随着学校的训练消磨而逐渐丧失
<ul>
<li>要允许与众不同且不被嘲笑</li>
<li>引导而不是可以创造，不失去这份创造力</li>
</ul>
</li>
<li>学会忽视他人对你的看法
<ul>
<li>有趣的人之所以有趣，是因为他们自己有一套关于成功的定义（在射中地方，画一个靶心），而不是接受别人的成功理念。</li>
<li>你对自己的投入学习的资源越多，就越了解自己。自我认知代表着巨大的力量。你需要找到最适合自己的位置，最适合自己的角色。想学焊接，就花钱去学，想学插花，就花钱去学，而不是受人意识干扰</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="拥抱未来">拥抱未来</h2>
<p>过去学个知识，干一辈子的时代已经渐渐远去了，<font color="blue">经历了多次科技革命的我们，正处在一个加速时期</font>，新工具出现越来越快，取代效应也越来越快。</p>
<blockquote>
<p>大量受规训的人毕业了被告知还要再学习就情绪上抵触，好在社会教做人，因为很快意识到市场和工具变化究竟有多快。当然也有从事简单重复工作的岗位，与再学习逐渐分离，但多数也随之甩去了改变生活境遇层次的机会。</p>
</blockquote>
<p>电出来的时候被认为是会带来灾难的巫术，无论你是欣喜还是焦虑，它终究会在未来的某一天不期而遇。市场不会因为禁用而整体不用。</p>
<p>靠人口和房子的粗旷式发展的大周期已经结束，人口下滑也是不可逆的趋势，中国正在经历劳动密集向效率提升的转型。时代需要新科技，新动能来救场。</p>
<p><font color="blue">人类历史从来不是人和工具之间的搏斗，而是人+工具替代人的演变。</font>当人类整体内大幅增加时，个人优势被抹平，苦痛会随之重新增加，立于潮头，意味着更少的竞争与更多的机会。保持竞争优势，亦不要被欲望收割，才能获得轻松幸福的生活。</p>
<h3 id="教育适配">教育适配</h3>
<p>我们小初中训练最多的死记硬背，心算，重复难度的刷题能力，这种反人性的规训是要进行反思的，不要成为一个按一定工序墨守成规的执行机器，这种能力20年后被人工智能淹没是大概率的事情。如何思考事物之间的关联，而不是只想快点看到老师的总结，面向未来学习。</p>
<p>越是在人工智能时代，越是要广泛的跨学科跨领域阅读。在人工智能时代，能准确描述你要的东西，也变得非常有价值，美学的认知和表达能力成为一大要素，说到底我们是商品社会，未来大众会越来越为美的东西买单，如果制作过程不再那么重要，那么懂美学的孩子就能做出更出色的产品。人和计算机是合作者的关系，要相信你的创造力。例如在一些大的问题的解决上，如一部电影的创意能打动人，这现在还是大模型/AI所做不到的。</p>
<p>个性和特长的培养也会显得比以往更为重要（一直重要，但更为重要了）。新时代的动手能力，就是配合基础学科及美学素养，从小锻炼使用现代工具辅助学习的能力。AI对技术的颠覆，对艺术的颠覆式必然的，正确使用工具对小孩整体帮助是大的，这里的<font color="dodgerblue">正确使用是纯耗时技巧/试错的工具化替代，是追问，反问，问答交流，而不是简单依赖AI来提供答案替代自我训练</font>。</p>
<h4 id="不要疯魔">不要疯魔</h4>
<p>不能因为反感死记硬背，就把所有的知识都给刨掉了，尽管知识在搜索和大模型里可以查到/回答。认为大模型工具能代替你提供答案，不用学了，会使用工具就可以，而放弃了刻苦的学习和思考过程，那真是走错了路。</p>
<p>小孩如果不用各种知识来进行学习，就像不进行训练一样，没有办法在大脑里形成新的神经网络连接，是<strong>不能凭空创造出创造力的。有价值的想象力不是胡思乱想的能力，想象力离不开见多识广</strong>。通过一定量必要的知识学习，作为一种预训练的方式，是非常重要的。</p>
<p>读书就是预训练，做题就是微调，被师傅批评就是校正对齐，通过考试和做题使你更容易使用知识。人工智能工具的发展是在降低使用的门槛，相当于科技平权（专家的技能下放给普通人拥有），最后比的还是人的创造力和解决问题的能力。</p>
<p>搜题搜答案的工具一定不要让小孩用，此类工具让小孩都是即时满足，没有耐心去花时间思考问题怎么解决。<em>做题的过程就是fine tuning，花半小时考虑不出来，相当于把你大脑里很多知识又重整了一遍</em>。</p>
<h1>中美AI研究差异</h1>
<blockquote>
<p>美国侧重基础研究，中国侧重解决方案。其实不仅AI，本世纪所有的科技发展，都在太平洋两岸衍生出不同的路径。</p>
</blockquote>
<ul>
<li>互联网浪潮美国对电商不热衷，线上消费渗透率一直上不去。中国几乎所有互联网公司都做过电商，渗透率冠绝全球，规模一度比2到11加总都高</li>
<li>移动互联网，中国凭借更好的网络环境，更鼓励创新的监管制度，直接跳过信用卡时代，进入数字支付时代</li>
<li>无人驾驶，美国侧重车的智能化，中国有更好的基建，路况，网络和交通规划，于是选了车路协同的路线</li>
<li>产业互联网，美国经济产业特点处于微笑曲线的两头，科技，互联网，金融占比高，加上人力昂贵，企业付费意愿强。中国集中在微笑曲线中段，作为世界工厂，场景丰富，产业链完整，政策支持，高效集中，产学研对接十分方便，技术验证更好落地。这样的大背景导致美国重攻基础研究，多是从技术起步，<font color="blue">中国优势在于场景多，需求多，往往是场景倒逼技术落地</font>。</li>
</ul>
<p><font color="blue"><u>中国民营企业才刚从艰苦奋斗的路上走出来，精打细算的习惯改变不了</u>，往往从市场需求产品需求开始，再慢慢投入科学家和基础研究，带动落地</font>。<font color="red">美国巨头钱不是问题，钱太多才是问题，砸钱做基础科学，既可以抢占科技高地，也需要冲淡垄断者的坏形象。</font></p>
<p>美国AI行业上一个爆款DeepMind的Alpha系列，就是先把技术做出来，赢围棋冠军，但商业落地慢慢探索，好几年后这项技术被用于破解蛋白质折叠结构难题，参与新药研发，才算英雄有用武之地。</p>
<p>中国用户早期很多用个人电脑自拍QQ头像，QQ团队就想，做个技术实现头像居中，解决这个问题后，逐步孵化出人脸检测，人像表情，智能P图等技术。用回产品，孵化出天天P图；人像美容技术再用到全民K歌，这个图像团队就是腾讯优图。还有美团的无人机，京东的智能供应链，都市跟主业投入有关。</p>
<h2 id="欧洲在哪">欧洲在哪?</h2>
<p>一句戏谑：美国人在创新，中国人在应用，欧洲人在立法。例如大模型商用基本只剩中美两个玩家。</p>
<p>当然中国当前一些科技领域也走在世界探索的前列，相对而言美国还是更强。</p>
<h2 id="一个词话三方">一个词话三方</h2>
<p>美国<font color="blur"><strong>虎</strong></font>（0-1，无知者无畏的自信去行动，想到就真敢去做），中国<font color="blur"><strong>卷</strong></font>（1-100），欧洲<font color="blur"><strong>守</strong></font>（保守高傲和磨叽）</p>
<h1>截止23年6月的评测报告</h1>
<p>一份来自清华大学的评测报告<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="清华大学 [大语言模型综合性能评估报告](https://mp.weixin.qq.com/s?__biz=MzI0NTUyNTM1OQ==&mid=2247485111&idx=1&sn=76d888b2c2f686f5313afbf573bf7292&chksm=e94c78b7de3bf1a1a4bac69eb7b68aa61d911bb4ffcbf994888d05e8b1ac0ad4c55925bc1dce)">[6]</span></a></sup></p>
<h2 id="综合得分评估">综合得分评估</h2>
<p>总得分=生成质量<em>70%+使用与性能</em>20%+安全与合规*10%</p>
<table>
<thead>
<tr>
<th>排名</th>
<th>大模型产品</th>
<th>加权总得分</th>
<th>生成质量（70%）</th>
<th>使用与性能</th>
<th>安全与合规（10%）</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>GPT-4</td>
<td>79.11</td>
<td>81.44</td>
<td>71.43</td>
<td>78.18</td>
</tr>
<tr>
<td>2</td>
<td>文心一言(v2.2)</td>
<td>76.18</td>
<td>76.98</td>
<td>72.38</td>
<td>78.18</td>
</tr>
<tr>
<td></td>
<td>ChatGPT 3.5</td>
<td>73.11</td>
<td>73.03</td>
<td>74.05</td>
<td>71.82</td>
</tr>
<tr>
<td>4</td>
<td>Claude(V1.3)</td>
<td>71.48</td>
<td>73.23</td>
<td>63.81</td>
<td>74.55</td>
</tr>
<tr>
<td>5</td>
<td>讯飞星火(V1.5)</td>
<td>66.67</td>
<td>66.87</td>
<td>64.76</td>
<td>69.09</td>
</tr>
<tr>
<td></td>
<td>通义千问(V1.0.3)</td>
<td>61.35</td>
<td>59.79</td>
<td>63.81</td>
<td>67.27</td>
</tr>
<tr>
<td>7</td>
<td>天工（V3.5)</td>
<td>61.16</td>
<td>64.51</td>
<td>50.48</td>
<td>59.09</td>
</tr>
</tbody>
</table>
<h2 id="中文能力点评">中文能力点评</h2>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="http://cdn.diffday.com/picgo/20230830145401.png" alt="大模型中文理解排名"></p>
<h2 id="简要评价">简要评价</h2>
<table>
<thead>
<tr>
<th>大模型产品</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4</td>
<td>高度灵活，高水平的跨领域专业知识</td>
<td>中文语义理解能力稍逊色，数据时效性弱</td>
</tr>
<tr>
<td>文心一言</td>
<td>语义理解能力和时效性强，内容安全把握细微</td>
<td>陷阱信息识别能力有待提高</td>
</tr>
<tr>
<td>ChatGPT 3.5</td>
<td>知识面覆盖广，响应迅速</td>
<td>同GPT4</td>
</tr>
<tr>
<td>Claude</td>
<td>内容多样性强，完整度高</td>
<td>响应速度慢，时效弱，不擅数学问题</td>
</tr>
<tr>
<td>讯飞星火</td>
<td>使用便捷，响应速度快，内容精炼</td>
<td>陷阱信息识别能力和知识专业化水平有待提高</td>
</tr>
<tr>
<td>通义千问</td>
<td>稳定性和准确度高，时效性强</td>
<td>历史，法律，数学相关问题表现欠佳</td>
</tr>
<tr>
<td>天工（昆仑万维-游戏/软件发行商）</td>
<td>出色的多轮对话能力，时效性强</td>
<td>响应速度慢，数学推理能力较弱</td>
</tr>
</tbody>
</table>
<h1>2023年10月OpenAI的价值观变动议论</h1>
<p>OpenAI悄然修改了其网站上列出的“核心价值观”，之前的价值观为“大胆”、“深思熟虑”、“朴实无华”、“影响力驱动”、“协作”和“以增长为导向”。</p>
<p>修改后为：</p>
<ul>
<li>聚焦通用人工智能</li>
<li>坚韧不拔、勇往直前</li>
<li>坚守规模化效应</li>
<li>制造出让人喜爱的东西</li>
<li>团队精神</li>
</ul>
<p>大家敏感，<u>是因为Core Values都能轻易更改，那还惘谈核心</u>，激起外界对于该公司在既定目标一致性和承诺方面的担忧。让人联想起谷歌从其核心价值页面中删除“不作恶”的时候，说明公司行事风格将会与以前不同了。</p>
<p>还有更一针见血的网友指出：貌似OpenAI经营者不理解<u>价值观</u>、<u>使命</u>、<u>目标</u>和<u>愿景</u>之间的区别。以前的价值观没问题，但修改后的价值观不是真正的价值观。它们是一些雄心勃勃的陈述的大杂烩。<font color="blur">当你需要做一些额外的工作来解释这些所谓的价值观</font>，就要想想选出的价值观是否堪配其位了，映衬出原来的深思熟虑也没做到，怪不得被移除，：）</p>
<h2 id="瞅瞅MVV概念">瞅瞅MVV概念</h2>
<blockquote>
<ul>
<li>
<p>愿景（Vision）是目标，使命（Mission）是意义[使命和意义也可以合一]，价值观（Values）是准则和文化（底线，行为准则和信仰）。</p>
<ul>
<li>使命更抽象宏大，愿景更具象可达 | 战略实现愿景，愿景支撑使命。</li>
</ul>
</li>
<li>
<p>文化就像空气，看不见，摸不着，但决定生死，且会吞噬战略。在企业打拼的过程中，是创始团队的认知凝结，也掺入后来团队共同打磨的认知沉淀，是结果。由于认同，会让团队内部沟通决策的成本大幅降低。</p>
</li>
</ul>
</blockquote>
<table>
<thead>
<tr>
<th>使命</th>
<th>愿景</th>
<th>战略</th>
</tr>
</thead>
<tbody>
<tr>
<td>公司为什么存在？</td>
<td>领导者希望公司发展成什么样？</td>
<td>击败现有及潜在竞争者的计划</td>
</tr>
<tr>
<td>- 为组织内所有决策提供前提<br>- 描述一个持久的事实<br>- 可是一个无限时期的解答<br>- 为内部和外部人员提供指导</td>
<td>- 指导战略和组织的发展<br>- 描述一个鼓舞人心的事实<br>- 可在一个特定时期内实现<br>- 主要为内部人员提供指导（有些口号也可提供给外部人员）</td>
<td>- 列出一系列举措以提供产品或服务，创造高于其成本的价值<br>- 描述公司战略选择的“价值方案”<br>- 随市场分析、消费者经验、试验而不断改善<br>- 最好严格限制在内部使用</td>
</tr>
</tbody>
</table>
<p>当然也有公司，每次战略规划或引入外部和尚念经时 总动这些概念的主义，但恰恰说明其先前沉淀思考的不足，反映变化和成长是好的，就怕把这个当有魔力的法宝（逆风逆水时这些虚的都没用，顺水推舟锦上添花还行）。</p>
<p>华为至今为人津津乐道的核心价值观还是以客户为中心（一种拉力），长期艰苦奋斗（一种推力）与以奋斗者为本（一种动力），没怎么变过。</p>
<h2 id="突发事件">突发事件</h2>
<ul>
<li>
<p>OpenAI的CEO在11.17日被董事会开除了，经过一个周末未收回成命，被金主爸爸微软收入麾下。</p>
</li>
<li>
<p>11.29日，原CEO SAM Altman又重回OpenAI担任CEO，微软在董事会获得了一个无投票权的观察员席位，能更深入了解OpenAI内部运作，但在重大决策中没有正式投票权，Maybe这只是治理结构变形的第一步，继续拭目以待吧。首席科学家Ilya已不再在董事会任职。在什么狗屁董事会审查工作结论出现之前，多方还是就此次宫斗避而不谈。还是静待Q*出现吧。</p>
</li>
<li>
<p>谷歌开发布会宣传自己Gemini吊打GPT4，竟然用编辑视频，劈柴哥在头部CEO中真是一个无能宵小之辈。</p>
<ul>
<li>
<p>2024.2月发布的Gemini1.5深陷种族主义和性别主义问题，文生图功能刻意拒绝生成白人形象，导致功能被迫下架。</p>
</li>
<li>
<p>公司创始人布林承认内部测试不充分，模型中有团队没完全理解的部分。Gemini1.5内部代号讽刺，金鱼（毕竟金鱼以记忆力短著称），只是作为扩大训练规模的一个尝试，没想到最后训练出来的模型有非常强的记忆力（百万token上下文窗口）</p>
<ul>
<li>两个创始人page远离聚光灯，在私人岛屿隐居，关注绝对隐私；brin一直保持相对公开形象，甚至成了派对动物。成活方式有着天壤之别。</li>
</ul>
</li>
<li>
<p>劈柴的离职估计不远了，本次gemini的发布成了科技界的汪峰，被同天的sora抢尽了风头。</p>
</li>
</ul>
</li>
</ul>
<h1>重要参考</h1>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">张俊林.<a target="_blank" rel="noopener" href="http://k.sina.com.cn/article_2674405451_9f68304b01901346f.html">由ChatGPT反思大语言模型（LLM）的技术精要</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">GLM团队.<a target="_blank" rel="noopener" href="https://keg.cs.tsinghua.edu.cn/glm-130b/zh/posts/glm-130b/">GLM-130B：开源的双语预训练模型</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">稀土掘金.<a target="_blank" rel="noopener" href="https://juejin.cn/post/7218014583863656503">Github Copilot 程序员下岗指南</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">张俊林 <a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2023-04-28-4">大语言模型的涌现能力——现象与解释</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">张俊林 <a target="_blank" rel="noopener" href="https://k.sina.cn/article_2674405451_9f68304b019013nky.html">炼制“大语言模型”的两个现象</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">清华大学 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzI0NTUyNTM1OQ==&amp;mid=2247485111&amp;idx=1&amp;sn=76d888b2c2f686f5313afbf573bf7292&amp;chksm=e94c78b7de3bf1a1a4bac69eb7b68aa61d911bb4ffcbf994888d05e8b1ac0ad4c55925bc1dce">大语言模型综合性能评估报告</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li></ol></div></div></article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src="https://cdn.diffday.com/picgo/diff_128.png" title="头像" alt="头像"><img class="post-copyright__author_img_front" src="https://cdn.diffday.com/picgo/diff_128.png" title="头像" alt="头像"></a><div class="post-copyright__author_name">DiffDay</div><div class="post-copyright__author_desc">We are here to be creators!</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://blog.diffday.com/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://blog.diffday.com/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html')">大语言模型LLM</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你的肯定与鼓励</span><ul class="reward-group"><li class="reward-item"><a href="http://cdn.diffday.com/picgo/wechatpay.jpg" target="_blank"><img class="post-qr-code-img" src="http://cdn.diffday.com/picgo/wechatpay.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="http://cdn.diffday.com/picgo/alipay-n.jpeg" target="_blank"><img class="post-qr-code-img" src="http://cdn.diffday.com/picgo/alipay-n.jpeg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://blog.diffday.com/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=大语言模型LLM&amp;url=https://blog.diffday.com/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html&amp;pic=http://cdn.diffday.com/picgo/20230408222056.png" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="rm.copyPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.diffday.com" target="_blank">Diffday</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/%E6%8A%80%E6%9C%AF/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>技术<span class="tagsPageCount">8</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="http://cdn.diffday.com/picgo/20240506152503.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/DevSecOps.html"><img class="prev-cover" src="http://cdn.diffday.com/picgo/20230315140348.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">DevSecOps</div></div></a></div><div class="next-post pull-right"><a href="/%E8%87%AA%E5%BE%8Bvs%E8%87%AA%E7%94%B1.html"><img class="next-cover" src="http://cdn.diffday.com/picgo/20230330165007.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">自律是一种阶级属性</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/DevSecOps.html" title="DevSecOps"><img class="cover" src="http://cdn.diffday.com/picgo/20230315140348.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-03-15</div><div class="title">DevSecOps</div></div></a></div><div><a href="/Serverless.html" title="Serverless简解"><img class="cover" src="http://cdn.diffday.com/picgo/20221208174734.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-12-08</div><div class="title">Serverless简解</div></div></a></div><div><a href="/%E5%A0%86%E6%8E%92%E5%BA%8F%E4%B8%8E%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F.html" title="堆排序与归并排序"><img class="cover" src="http://cdn.diffday.com/picgo/20221024110942.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-01-16</div><div class="title">堆排序与归并排序</div></div></a></div><div><a href="/%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%E5%8A%A0%E5%AF%86.html" title="敏感信息加密"><img class="cover" src="http://cdn.diffday.com/picgo/20230314172247.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-03-14</div><div class="title">敏感信息加密</div></div></a></div><div><a href="/%E6%95%B0%E5%AD%97%E5%8C%96%E6%97%B6%E4%BB%A3%E5%9B%9E%E7%9C%8B%E8%BF%90%E7%BB%B4.html" title="数字化时代回看运维"><img class="cover" src="http://cdn.diffday.com/picgo/20230526183811.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-05-26</div><div class="title">数字化时代回看运维</div></div></a></div><div><a href="/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E6%B5%85%E8%BF%B0.html" title="数据倾斜浅述"><img class="cover" src="http://cdn.diffday.com/picgo/20221109111839.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-10-10</div><div class="title">数据倾斜浅述</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="anzhiyufont anzhiyu-icon-comments"></i><span> 评论</span></div><div class="comment-randomInfo"><a onclick="anzhiyu.addRandomCommentInfo()" href="javascript:void(0)">匿名评论</a><a href="/privacy" style="margin-left: 4px">隐私政策</a></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div><div class="comment-barrage"></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">惊喜与惊醒</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">NLP研究范式转变</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E4%B8%A4%E9%98%B6%E6%AE%B5%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">从深度学习到两阶段训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%9F"><span class="toc-number">2.1.1.</span> <span class="toc-text">深度学习期</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E9%98%B6%E6%AE%B5%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.2.</span> <span class="toc-text">两阶段训练大模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E6%9D%A5%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">2.1.3.</span> <span class="toc-text">带来的影响</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%AD%E9%97%B4%E4%BB%BB%E5%8A%A1%E6%B6%88%E4%BA%A1"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">中间任务消亡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF%E7%BB%9F%E4%B8%80"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">技术路线统一</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E5%88%B0%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="toc-number">2.2.</span> <span class="toc-text">预训练到通用人工智能</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ChatGPT"><span class="toc-number">2.2.1.</span> <span class="toc-text">ChatGPT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ChatGPT%E6%94%B9%E5%8F%98%E4%BA%86GPT-3-5%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">ChatGPT改变了GPT-3.5什么？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM%E7%9A%84%E7%9F%A5%E8%AF%86%E6%9E%84%E6%88%90"><span class="toc-number">2.2.2.</span> <span class="toc-text">LLM的知识构成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AD%98%E5%8F%96%E7%9F%A5%E8%AF%86"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">如何存取知识</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM%E7%9A%84%E8%A7%84%E6%A8%A1%E6%95%88%E5%BA%94"><span class="toc-number">2.2.3.</span> <span class="toc-text">LLM的规模效应</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%8A%E6%A8%A1%E5%9E%8B%E5%81%9A%E5%B0%8F%E4%BC%9A%E5%BD%B1%E5%93%8DLLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B%E4%B9%88"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">把模型做小会影响LLM的涌现能力么</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E7%9A%84%E7%A8%80%E7%96%8F%E5%8C%96"><span class="toc-number">2.2.4.</span> <span class="toc-text">Transformer的稀疏化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92"><span class="toc-number">2.2.5.</span> <span class="toc-text">人机交互</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%A2%9E%E5%BC%BALLM%E7%9A%84%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B"><span class="toc-number">2.2.6.</span> <span class="toc-text">如何增强LLM的推理能力</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">算力约束下的最优培养策略</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">为何是OpenAI</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-OpenAI-Not-Google"><span class="toc-number">4.1.</span> <span class="toc-text">Why OpenAI Not Google</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Step-By-Step"><span class="toc-number">4.2.</span> <span class="toc-text">Step By Step</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E5%BF%83%E6%8F%92%E6%9F%B3"><span class="toc-number">4.2.1.</span> <span class="toc-text">无心插柳</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%A2%E5%AF%B9AI%E6%81%90%E6%83%A7%E7%9A%84%E8%A7%A3%E7%AD%94"><span class="toc-number">4.2.2.</span> <span class="toc-text">面对AI恐惧的解答</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">成本与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%99%BA%E7%AE%97%E9%9B%86%E7%BE%A4%E6%88%90%E6%9C%AC"><span class="toc-number">5.1.</span> <span class="toc-text">智算集群成本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E6%8C%91%E6%88%98"><span class="toc-number">5.2.</span> <span class="toc-text">技术挑战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%82%E5%90%88%E6%99%AE%E9%80%9A%E7%8E%A9%E5%AE%B6%E7%9A%84%E7%82%BC%E4%B8%B9"><span class="toc-number">5.3.</span> <span class="toc-text">适合普通玩家的炼丹</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">冲击</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A5%87%E7%82%B9%E4%B8%B4%E8%BF%91"><span class="toc-number">6.1.</span> <span class="toc-text">奇点临近</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%B1%E4%B8%9A%EF%BC%8C%E9%A2%84%E8%A8%80%E8%BF%98%E6%98%AF%E8%B0%8E%E8%A8%80"><span class="toc-number">6.2.</span> <span class="toc-text">失业，预言还是谎言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="toc-number">6.2.1.</span> <span class="toc-text">代码生成大模型的不足</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E8%A7%86%E4%BA%BA%E6%80%A7"><span class="toc-number">6.2.2.</span> <span class="toc-text">正视人性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%90%E4%B8%BA%E4%B8%80%E4%B8%AA%E6%97%A0%E6%B3%95%E8%A2%ABAI%E5%8F%96%E4%BB%A3%E7%9A%84%E4%BA%BA"><span class="toc-number">6.2.3.</span> <span class="toc-text">成为一个无法被AI取代的人</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%A5%E6%8A%B1%E6%9C%AA%E6%9D%A5"><span class="toc-number">6.3.</span> <span class="toc-text">拥抱未来</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%99%E8%82%B2%E9%80%82%E9%85%8D"><span class="toc-number">6.3.1.</span> <span class="toc-text">教育适配</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E8%A6%81%E7%96%AF%E9%AD%94"><span class="toc-number">6.3.1.1.</span> <span class="toc-text">不要疯魔</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">中美AI研究差异</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AC%A7%E6%B4%B2%E5%9C%A8%E5%93%AA"><span class="toc-number">7.1.</span> <span class="toc-text">欧洲在哪?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E8%AF%8D%E8%AF%9D%E4%B8%89%E6%96%B9"><span class="toc-number">7.2.</span> <span class="toc-text">一个词话三方</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">截止23年6月的评测报告</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%BC%E5%90%88%E5%BE%97%E5%88%86%E8%AF%84%E4%BC%B0"><span class="toc-number">8.1.</span> <span class="toc-text">综合得分评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%AD%E6%96%87%E8%83%BD%E5%8A%9B%E7%82%B9%E8%AF%84"><span class="toc-number">8.2.</span> <span class="toc-text">中文能力点评</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E8%A6%81%E8%AF%84%E4%BB%B7"><span class="toc-number">8.3.</span> <span class="toc-text">简要评价</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">9.</span> <span class="toc-text">2023年10月OpenAI的价值观变动议论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9E%85%E7%9E%85MVV%E6%A6%82%E5%BF%B5"><span class="toc-number">9.1.</span> <span class="toc-text">瞅瞅MVV概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AA%81%E5%8F%91%E4%BA%8B%E4%BB%B6"><span class="toc-number">9.2.</span> <span class="toc-text">突发事件</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">10.</span> <span class="toc-text">重要参考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/%E5%8E%86%E5%8F%B2%E7%9A%84%E6%A3%8B%E5%B1%80.html" title="历史的棋局"><img src="https://cdn.diffday.com/picgo/20231008103727.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="历史的棋局"/></a><div class="content"><a class="title" href="/%E5%8E%86%E5%8F%B2%E7%9A%84%E6%A3%8B%E5%B1%80.html" title="历史的棋局">历史的棋局</a><time datetime="2024-05-12T00:25:45.000Z" title="更新于 2024-05-12 08:25:45">2024-05-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E4%B9%9D%E5%9E%8B%E4%BA%BA%E6%A0%BC.html" title="九型人格"><img src="http://cdn.diffday.com/picgo/20210816153515.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="九型人格"/></a><div class="content"><a class="title" href="/%E4%B9%9D%E5%9E%8B%E4%BA%BA%E6%A0%BC.html" title="九型人格">九型人格</a><time datetime="2024-05-11T05:58:15.000Z" title="更新于 2024-05-11 13:58:15">2024-05-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%A4%AE%E8%A1%8C%E8%B4%9F%E5%80%BA%E8%A1%A8%E8%A7%A3%E8%AF%BB.html" title="央行负债表解读"><img src="http://cdn.diffday.com/picgo/20211116152006.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="央行负债表解读"/></a><div class="content"><a class="title" href="/%E5%A4%AE%E8%A1%8C%E8%B4%9F%E5%80%BA%E8%A1%A8%E8%A7%A3%E8%AF%BB.html" title="央行负债表解读">央行负债表解读</a><time datetime="2024-05-09T09:39:10.000Z" title="更新于 2024-05-09 17:39:10">2024-05-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%B0%91%E4%B8%8D%E8%AF%BB%E6%B0%B4%E6%B5%92.html" title="少不读水浒"><img src="http://cdn.diffday.com/picgo/20220221171815.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="少不读水浒"/></a><div class="content"><a class="title" href="/%E5%B0%91%E4%B8%8D%E8%AF%BB%E6%B0%B4%E6%B5%92.html" title="少不读水浒">少不读水浒</a><time datetime="2024-05-08T03:05:47.000Z" title="更新于 2024-05-08 11:05:47">2024-05-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%8D%93%E6%9C%89%E6%88%90%E6%95%88%E7%9A%84%E7%AE%A1%E7%90%86%E8%80%85.html" title="卓有成效的管理者"><img src="http://cdn.diffday.com/picgo/卓有成效的管理者.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="卓有成效的管理者"/></a><div class="content"><a class="title" href="/%E5%8D%93%E6%9C%89%E6%88%90%E6%95%88%E7%9A%84%E7%AE%A1%E7%90%86%E8%80%85.html" title="卓有成效的管理者">卓有成效的管理者</a><time datetime="2024-05-06T10:03:18.000Z" title="更新于 2024-05-06 18:03:18">2024-05-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E8%AE%A9%E5%A4%A7%E8%84%91%E8%87%AA%E7%94%B1.html" title="让大脑自由"><img src="http://cdn.diffday.com/picgo/20230801234015.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="让大脑自由"/></a><div class="content"><a class="title" href="/%E8%AE%A9%E5%A4%A7%E8%84%91%E8%87%AA%E7%94%B1.html" title="让大脑自由">让大脑自由</a><time datetime="2024-05-06T07:49:27.000Z" title="更新于 2024-05-06 15:49:27">2024-05-06</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2018 - 2024 By <a class="footer-bar-link" href="/" title="DiffDay" target="_blank">DiffDay</a></div></div><div id="footer-type-tips"></div><div class="js-pjax"><script>function subtitleType () {
  getScript('https://sdk.jinrishici.com/v2/browser/jinrishici.js').then(() => {
    jinrishici.load(result =>{
      if (false) {
        const sub = []
        const content = result.data.content
        sub.unshift(content)
        window.typed = new Typed('#footer-type-tips', {
          strings: sub,
          startDelay: 300,
          typeSpeed: 150,
          loop: true,
          backSpeed: 50,
        })
      } else {
        document.getElementById('footer-type-tips').innerHTML = result.data.content
      }
    })
  })
}

if (false) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.jsdelivr.net/npm/typed.js/dist/typed.umd.min.js').then(subtitleType)
  }
} else {
  subtitleType()
}
</script></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a><a class="footer-bar-link" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" title="粤ICP备-18103410号">粤ICP备-18103410号</a><a class="footer-bar-link cc" href="/copyright" title="cc协议"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-by-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nc-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nd-line"></i></a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">130</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">48</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">14</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/archives/"><span> 归档</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/tags/"><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本站</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/copyright/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 版权协议</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="anzhiyufont anzhiyu-icon-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script async src="/anzhiyu/random.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  //if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js').then(runMermaid)
  }

  anzhiyu.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'uwklgzjj0IepCMXNYCU3t9Ah-9Nh9j0Va',
      appKey: 'jRMVc9QS0SBr1L9heA2V0m4q',
      avatar: 'mp',
      serverURLs: 'https://uwklgzjj.lc-cn-e1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !true) {
    if (true) anzhiyu.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><input type="hidden" name="page-type" id="page-type" value="post"></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=mp'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${anzhiyu.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://uwklgzjj.lc-cn-e1-shared.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'uwklgzjj0IepCMXNYCU3t9Ah-9Nh9j0Va',
        "X-LC-Key": 'jRMVc9QS0SBr1L9heA2V0m4q',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "无法获取评论，请确认相关配置是否正确"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script>var visitorMail = "visitor@anheyu.com";
</script><script async data-pjax src="https://cdn.jsdelivr.net/npm/anzhiyu-theme-static/waterfall/waterfall.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/anzhiyu-theme-static/icon/ali_iconfont_css.min.css"><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: true,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', 'G-X0MW6B53JB', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.jsdelivr.net/npm/anzhiyu-theme-static/accesskey/accesskey.min.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>