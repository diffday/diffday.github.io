<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大语言模型LLM | Diffday</title><meta name="author" content="DiffDay"><meta name="copyright" content="DiffDay"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ChatGPT  GPT:Generative Pre-trained Transformer 科技部长的金句：踢足球都是盘点，射门，但是要做到梅西那么好也不容易  惊喜与惊醒大语言模型的效果好到令人咋舌，我们距离LLM的认知和发展理念，距离世界最先进的想法，差得有点远  Bert出现后1~2年间"><link rel="shortcut icon" href="https://cdn.diffday.com/picgo/bitbug_favicon32.ico"><link rel="canonical" href="https://blog.diffday.com/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="UaMvLIOYuNWHgVq42dlN49RoniU73U6SoqoRnEcit9E"/><meta name="baidu-site-verification" content="code-DN25SBuR8T"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b5c9889dd454fff053fb5b93fc1b2ac7";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-X0MW6B53JB"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-X0MW6B53JB');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: DiffDay","link":"链接: ","source":"来源: Diffday","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大语言模型LLM',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-06 18:57:32'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/diff_128.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">114</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Diffday"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/diff_128.png"/><span class="site-name">Diffday</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">大语言模型LLM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-23T09:36:00.000Z" title="发表于 2023-03-23 17:36:00">2023-03-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-06T10:57:32.000Z" title="更新于 2023-06-06 18:57:32">2023-06-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%BD%91%E6%96%87%E7%B2%BE%E6%91%98/">网文精摘</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>26分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大语言模型LLM"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230408222056.png" alt="ChatGPT催生AI再一春"></p>
<p>ChatGPT</p>
<blockquote>
<p>GPT:Generative Pre-trained Transformer</p>
<p><font color="blue">科技部长的金句：踢足球都是盘点，射门，但是要做到梅西那么好也不容易</font></p>
</blockquote>
<h1 id="惊喜与惊醒"><a href="#惊喜与惊醒" class="headerlink" title="惊喜与惊醒"></a>惊喜与惊醒</h1><p>大语言模型的效果好到令人咋舌，我们距离LLM的认知和发展理念，距离世界最先进的想法，差得有点远</p>
<ul>
<li>Bert出现后1~2年间，国内追赶技术很快，也提出了一些改进模型</li>
<li>分水岭在GPT-3，即为2020年中，体现了LLM应往何处去的发展理念，全球看中的人很少，梯队明显<ul>
<li><font color="red"><u>包括Google在内，对于LLM发展理念的理解，都落后OpenAI一个身位</u></font>（半年到一年的时间）</li>
<li>国内可能落后2年左右</li>
</ul>
</li>
</ul>
<blockquote>
<p>鹰酱的风格是进化论模式，各个方向上各个公司都搞低成本试探进攻，让企业家去承担试错成本。进化是允许犯错的，甚至是进化不可少的前提</p>
</blockquote>
<p><font color="blue"><strong>最难的事情 : 方向可行性</strong>，已经被蹚出来了</font>（此正是技术最难的一点）</p>
<ul>
<li><p>我方堆资源，集中力量办大事的优势可以发挥</p>
</li>
<li><p>商业的竞争，开源的平替也在出现（甚至可能是故意的泄露）</p>
</li>
<li><p>老美限制我们，<font color="red">软件上挡不住，那就硬件上挡</font></p>
<span id="more"></span></li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230321184942.png"></p>
<blockquote>
<p>A800是A100阉割特供版，计算性能相似，数据传输速度降低30%，影响AI集群训练速度和效果，还缺货，一次只能采购数百片</p>
</blockquote>
<h1 id="NLP研究范式转变"><a href="#NLP研究范式转变" class="headerlink" title="NLP研究范式转变"></a>NLP研究范式转变</h1><h2 id="从深度学习到两阶段训练模型"><a href="#从深度学习到两阶段训练模型" class="headerlink" title="从深度学习到两阶段训练模型"></a>从深度学习到两阶段训练模型</h2><h3 id="深度学习期"><a href="#深度学习期" class="headerlink" title="深度学习期"></a>深度学习期</h3><ul>
<li><p>由大量改进LSTM模型及少量改进的CNN模型作为典型的特征抽取器</p>
</li>
<li><p>以<font color="blue">Sequence to Sequence（或叫encoder-decoder亦可）+Attention</font>作为各种具体任务典型的总体技术框架</p>
</li>
</ul>
<p>在这些技术下，研究目标归纳为<u>如何有效增加模型层深或模型参数容量</u>。就是往encoder-decoder里不断叠加更深的LSTM或CNN层。</p>
<p>但<font color="blue">受限于有限的训练数据总量（不够匹配模型容量增加）</font>和特征抽取器有限的表达能力（不能吸收数据里蕴含的知识），最终这条路径相较于飞深度学习方法并没有出现碾压式的优势</p>
<blockquote>
<p>三元或四元甚至更高阶的模型是不是能覆盖所有语言现象。答案是不行</p>
<p>上下文之间相关性可能跨度非常大，甚至可以从一个段落到另一个段落</p>
</blockquote>
<h3 id="两阶段训练大模型"><a href="#两阶段训练大模型" class="headerlink" title="两阶段训练大模型"></a>两阶段训练大模型</h3><p>Bert和GPT模型出现后，在学术研究和工业应用角度看，都带来了一个技术飞跃，子领域的技术方法和框架日趋统一</p>
<blockquote>
<p>Bert出现一年左右，技术栈就基本全线收敛到此二位上。</p>
<p>图像领域预训练模型（vision transformer）应用到下游任务，带来的效果收益，远不如Bert&#x2F;GPT应用在NLP下游任务那么显著，要是蹚通了，图像处理的各个子研究领域可能也会逐步消失，直接完成终端任务</p>
</blockquote>
<h3 id="带来的影响"><a href="#带来的影响" class="headerlink" title="带来的影响"></a>带来的影响</h3><h4 id="中间任务消亡"><a href="#中间任务消亡" class="headerlink" title="中间任务消亡"></a>中间任务消亡</h4><p>中文分词，词性标注，命名实体识别（NER），句法分析，指代消解，语义Parser等，这类任务不是解决任务的实际需求，但作为解决任务的中间阶段或辅助阶段存在。而用户其实只关心最终具体任务有没有干好。</p>
<p>通过大量的预训练，Bert&#x2F;GPT已经把这些中间任务作为语言学特征，吸收到了Transformer参数里，无需对中间过程专门建模，可端到端直接解决最终任务。</p>
<blockquote>
<p>在技术发展的早期阶段，很难一步做好有难度的最终任务，科研人员就把难题分而治之</p>
</blockquote>
<h4 id="技术路线统一"><a href="#技术路线统一" class="headerlink" title="技术路线统一"></a>技术路线统一</h4><p>最终任务分类：NLU+NLG</p>
<p>NLU：文本分类，句子相似性计算，情感倾向判断，意图识别等，都是分类任务。</p>
<blockquote>
<p>统一到了Bert为代表的“双向语言模型预训练”+应用fine-tuning的模式</p>
</blockquote>
<p>NLG：聊天机器人，翻译，文本摘要，问答系统等</p>
<blockquote>
<p>统一到了GPT-2为代表的“自回归语言模型（从左到右单向语言模型）+zero&#x2F;few shot prompt”的模式</p>
</blockquote>
<p>绝大多数人当时都低估了GPT这条路线的潜力，视线中心都聚焦到了Bert模式上。</p>
<h2 id="预训练到通用人工智能"><a href="#预训练到通用人工智能" class="headerlink" title="预训练到通用人工智能"></a>预训练到通用人工智能</h2><blockquote>
<p>从GPT-3以后，尚在加速演进</p>
</blockquote>
<h3 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h3><p>ChatGPT是触发这次范型转换的关键点，在InstructGPT出现前，LLM其实出于过渡期。</p>
<blockquote>
<p>ChatGPT最惊艳和最大的贡献是基本<font color="blue">实现了让LLM适配人的命令表达方式，给出了很好的解决方案，增加了易用性和用户体验</font></p>
<p><font color="blue">证明了可以去直接追求理想的LLM模型，未来的技术趋势应是越来越大的LLM模型，增加预训练数据的多样性</font></p>
</blockquote>
<ul>
<li><p>预训练模型早期，人们普遍更看好Bert一些</p>
<ul>
<li>fine-tuning方式解决下游任务，Bert&gt;GPT</li>
<li><font color="red"><u>Fine-tuning效果占优的领域是因为领域训练数据量大，从数据安全角度，fine-tuning还没那么快消失，但已经不是潮流了</u></font></li>
</ul>
</li>
<li><p>随着技术发展，目前规模最大的LLM模型，几乎清一色类似GPT-3的模式，背后有一定的必然性</p>
<ul>
<li><p><font color="blue">NLG表现形式可兼容NLU</font>，反之则不行。分类问题可转换成让LLM生成对应类别字符串，Google的T5模型，形式上就统一了NLU+NLG的外在表现形式。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230322170650.png" style="zoom:50%;" />
</li>
<li><p>Zero&#x2F;few shot promot方式做好任务，采取GPT模式</p>
<ul>
<li>数据是海量的，要吸收知识，需非常多的参数来存储只是，必是巨无霸模型</li>
<li>模型规模巨大，有能力做出及改动这个模型参数的机构必然少</li>
<li><font color="red">就算把模型开源出来，中小机构和个人也无力部署，更不用说用fine-tuning这种模式去修改模型参数了</font></li>
<li><font color="blue">LLM as Service的模式运行</font>，超大模型一定会走向AGI（人造通用智能）</li>
<li>ChatGPT用Instruct取代了prompting，由此带来新的技术范式转换</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="ChatGPT改变了GPT-3-5什么？"><a href="#ChatGPT改变了GPT-3-5什么？" class="headerlink" title="ChatGPT改变了GPT-3.5什么？"></a>ChatGPT改变了GPT-3.5什么？</h4><blockquote>
<p>GPT-1学习资料5G，参数1.17亿</p>
<p>GPT-2学习资料40G，参数15亿</p>
<p>GPT-3学习资料45T，参数1750亿</p>
</blockquote>
<p>GPT有了海量知识，但回答形式和内容却不受约束，因为它知道的太多了。见到了一个人几辈子都读不完的资料，会随意联想，像一只脑容量超级大的鹦鹉，如何指挥它成了一个目标。</p>
<p>ChatGPT注入了人类偏好知识，什么是好的回答，什么是不好的。如详细回答是好的，带有歧视内容的回答是不好的，人类对回答质量好坏的偏好，用对话模板去矫正其开卷有益时学到的不规范习惯（跟教鹦鹉说话一个道理），通过reward-model反馈给LLM数据，得到一个懂得人话，比较礼貌的LLM。</p>
<blockquote>
<p>用人工专门写好的优质对话范例让GPT去接龙</p>
</blockquote>
<h3 id="LLM的知识构成"><a href="#LLM的知识构成" class="headerlink" title="LLM的知识构成"></a>LLM的知识构成</h3><blockquote>
<p>Transformer是足够强大的特征抽取器，尚不需做特别的改进，那它学到了什么？</p>
</blockquote>
<p>语言类知识和世界知识</p>
<ul>
<li>语言类知识是指语法，词性，句法，语言等有助于人类或机器理解的自然语言知识</li>
<li>世界知识指发生在这个世界上的一些真实事件和常识性知识</li>
</ul>
<p>对于Bert类型的语言模型来说，只<u>用1000w到1亿单词的语料，就能学好句法语义等语言学知识</u>；<font color="blue">事实类知识要更多的训练数据</font>。</p>
<p>随着Transformer模型层深的增加，能学到的知识数据以指数级增加，把<font color="red">模型看作是以模式参数体现的隐式知识图谱</font>，一点也不违和。</p>
<h4 id="如何存取知识"><a href="#如何存取知识" class="headerlink" title="如何存取知识"></a>如何存取知识</h4><ul>
<li>多头注意力（MHA）占了参数总量的1&#x2F;3，用于计算单词或知识间的相关强度，对全局信息进行集成，建立知识间的联系，大概率不会存储具体的知识点</li>
<li>FFN（Feed Forward Network）结构占了剩余2&#x2F;3，承担主体知识的存储。FFN的输入层其实是某个单词对应的MHA的输出结果Embedding，将整个句子有关的输入上下文集成到一起的Embedding，代表整个输入句子的整体信息</li>
<li>Transformer低层对句子表层模式做出反应，高层对语义模式做出反应。也就是<font color="blue">低层FFN存储语法，句法等表层知识；中层和高层存储语义及事实概念知识</font></li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230322170801.png"></p>
<h3 id="LLM的规模效应"><a href="#LLM的规模效应" class="headerlink" title="LLM的规模效应"></a>LLM的规模效应</h3><p>目前效果最好的LLM模型，参数规模大都超过了千亿（100B），如OpenAI的GPT-3规模175B，Google的LaMDA规模540B，华为盘古模型200B，百度文心260B，随着模型不断增长，会发生什么？</p>
<ul>
<li><p>研究证明，越大的LLM模型学习效率越高，学到了更多知识，任务效果更好。多数NLU任务，都是知识密集型任务，近两年都在模型规模增长下获得了极大的效果提升。</p>
</li>
<li><p>模型规模是解锁LLM新能力的关键，出现某种涌现能力带来意想不到的精彩，如chatGPT的推理能力。</p>
<blockquote>
<p>思维链是典型的增强LLM推理能力的技术，流利性也是在规模上得以解决的。</p>
<p><font color="red">上下文学习里出现的涌现效应，等价于隐式的微调</font>，但如何有效尚未搞明白</p>
</blockquote>
</li>
</ul>
<h3 id="Transformer的稀疏化"><a href="#Transformer的稀疏化" class="headerlink" title="Transformer的稀疏化"></a>Transformer的稀疏化</h3><p>目前规模最大的LLM中，相当比例的模型采取了稀疏结构，如GPT-3，PaLM，好处是它可以极大减少LLM的训练时间和在线推理时间</p>
<p>有研究表明，标准的Dense Transformer在训练和推理时，它本身也是稀疏激活的，既然如此，不如直接迁移到稀疏模型</p>
<p>随着模型越大，稀疏模型带来的收益越明显</p>
<h3 id="人机交互"><a href="#人机交互" class="headerlink" title="人机交互"></a>人机交互</h3><blockquote>
<p>从In Context Learning到Instruct理解</p>
</blockquote>
<p>Zero shot prompt是Instruct的早期叫法，内涵一致，具体做法不同</p>
<ul>
<li>早期Zero shot prompt实际上就是不知道怎么表达一个任务才对，就换不同的单词或句子，反复尝试好的任务表达方式。这种方式已经被证明是在拟合训练数据的分布</li>
<li>Instruct做法则是给定命令表达语句，试图让LLM理解它，尽管表面都是任务的表述，但思路是不同的</li>
</ul>
<p>In Context Learning和Few shot prompt意思类似，就是<font color="red">给LLM几个示例做范本，然后让LLM解决新问题</font></p>
<ul>
<li>In Context Learning也可以理解为某项任务的描述（用例子来具象表达任务命令），<font color="blue">只是Instruct是一种更抽象的描述形式</font></li>
</ul>
<blockquote>
<p>LLM用来生成Instruct效果很不错，在一些任务上超过人类的表现，所以Prompt engineer也是一个不长久的职位</p>
</blockquote>
<ul>
<li><font color="blue">Fine-tuning和In Context Learning表面看似都提供了一些例子给LLM，但两者有质上的差别</font><ul>
<li>Fine-tuning拿这些例子当训练数据，用反向传播去修正LLM的模型参数</li>
<li>但In Context Learning只是拿出例子让LLM看了一眼，并没有根据例子去修正参数，就要求它去预测新例子（正是In Context Learning的神奇之处，尚无清晰的原理解释）</li>
</ul>
</li>
</ul>
<h3 id="如何增强LLM的推理能力"><a href="#如何增强LLM的推理能力" class="headerlink" title="如何增强LLM的推理能力"></a>如何增强LLM的推理能力</h3><p>咱们通常不会因为一个人单靠记忆力强，就说这个人很聪明，还要看他是否有强的推理能力，推理能力是智力水平更佳的标准。强大推理能力也是让用户认可LLM的心理基础。</p>
<blockquote>
<p>推理能力的本质是综合运用很多知识，去推导出新的知识或新结论</p>
</blockquote>
<p>在LLM推理方面相关的工作和研究，可归为4大类</p>
<ul>
<li><p>基于Prompt的方法，通过合适的提示语或文本，更好地激发LLM本身就具有的推理能力，google在这个方面做了大量很有成效的工作</p>
<ul>
<li>更好的展示出能力的技术方法，直接在问题上追加辅助推理Prompt，在众多领域都很有效<ul>
<li>第一阶段在提问的问题上追加“Let’s think step by step”这句提示语，LLM会输出具体的推理过程；第二阶段，在第一阶段的问题后，拼接LLM输出的具体推理过程，并再追加Prompt。如此简单的操作，却可以大幅增加LLM在各项推理任务中的效果，比如在数学推理测试集GSM8K上，加上提示语后，推理准确率直接从原先的10.4%提升到了40.4%，可谓神奇。（猜测预训练数据里面存在大量的此种数据，提示语激发LLM模糊得“回忆”起某些例子的推导步骤）</li>
</ul>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230322171145.png"></p>
</li>
<li><p>COT</p>
<p>标准的COT由人工来写推理步骤，而Zero-shot COT大概是通过提示语，激活了记忆中某些包含推理步骤的示例。人工给出的示例，准确性是有保障的，所以自然标准CoT效果会更好。</p>
<blockquote>
<p>最早的COT概念文章发表于22年1月，虽然做法简单，但应用COT后模型推理能力得到了巨大的提升</p>
</blockquote>
<p>意思是<font color="blue">让LLM明白一个道理：在推理过程中，步子不要迈的太大，化大问题为小问题，积小胜为大胜</font></p>
<p>COT提出不久，很快在22年3月，一种被称为“Self-Consistency”的改进技术继续助力提升准确率，它要求LLM输出多个不同的推理过程和答案，然后用投票的方式选出最佳答案，将GSM8K测试集准确率提高到83%左右。简答的方法往往蕴含着深刻的道理。虽然COT起效仍有黑盒的味道。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230322172430.png"></p>
</li>
<li><p>Least-to-most prompting，应用分治的思想，将一个复杂的推理问题，分解成若干更易解决的子问题，应证了COT的工作模式。<font color="blue">要解决Final Q问题，先把原始问题和Prompt交给LLM，让LLM给出最终问题的前置子问题sub Q，然后用原始问题拼接子问题sub Q及答案，再去问LLM最终问题Final Q</font></p>
</li>
<li><p>在预训练过程中<u><strong>引入程序代码，和文本一起参与训练</strong></u>，这应是OpenAI实践出来的思路</p>
<ul>
<li>体现出一种通过增强多样性的训练数据，来直接增强推理能力的思路</li>
<li>为何预训练模型可以从代码中获得额外的推理能力，确切原因未知。<font color="red">可能开始只是尝试从文本生成代码，而代码中往往包含很多文本注释，本质上类似于预训练模型做了两种数据的多模态对齐工作</font></li>
<li>支持越来越多的任务类型，主要是通过增加LLM预训练数据的多样性来达成</li>
</ul>
</li>
</ul>
<h1 id="算力约束下的最优培养策略"><a href="#算力约束下的最优培养策略" class="headerlink" title="算力约束下的最优培养策略"></a>算力约束下的最优培养策略</h1><p>假设用于训练LLM的算力总预算（如多少GPU天）给定，是应多增加数据量，减少模型参数呢，还是说数据量和模型规模同时增加，减少训练步数呢？</p>
<p><font color="blue"> OpenAI选择了同时增加训练数据量和模型参数，但采用早停策略来减少训练步数的方案</font></p>
<ul>
<li><font color="red">且优先增加模型参数，然后才是模型数据量</font></li>
<li>假如算力预算增加了10倍，那么应增加5.5倍的模型参数量，1.8倍的训练数据量，此时模型效果最佳</li>
<li>单调增加模型参数，固定住训练数据量，这个做法也是不对的，限制了模型的潜力</li>
</ul>
<h1 id="为何是OpenAI"><a href="#为何是OpenAI" class="headerlink" title="为何是OpenAI"></a>为何是OpenAI</h1><p>胜在一开始就自我定位比较高，要做出人物无关超大型LLM，以生成一切的方式解决各种实际问题，且应能听懂人类的命令。不受外界干扰态度坚定不移。</p>
<ul>
<li>GPT-1比Bert出来更早，Bert证明了双向语言模型对于很多NLU任务，效果比GPT这种单向语言模型更好，尽管如此，GPT-2也没有切换技术路线，且开始尝试zero&#x2F;few shot prompt。因效果比Bert+fine-tuning差的比较远，所以大家都没太当回事，甚至不理解它为什么要始终坚持走单向语言模型的路线。</li>
<li>GPT-3展示出不错的zero&#x2F;few shot prompt能力，后面技术差距从这里拉开，再往后是InstructGPT和ChatGPT</li>
</ul>
<blockquote>
<p>OpenAI的股权设计很特别，不受任何股东制约，投资者没有控制权，协议上是一种债的结构，赚完2万亿，接下来不再盈利了，一切回归社会</p>
</blockquote>
<p>OpenAI首席科学家Ilya Sutskever生于俄罗斯，长大于以色列，十多岁岁父母移民到了加拿大。从小就一直想搞清楚意识（consciousness）这个东西，对一切能助其了解意识的东西感兴趣，AI对其就是一个好的切入点，他有一个观点，你能高效压缩信息，你一定已得到知识（这与自己对模型的表述不谋而合）。在OpenAI建设引领中他和组织坚信两件事</p>
<ol>
<li>模型架构，它要足够深，bigness is betterness，只要有算力，只要有数据，越大越好。在OpenAI早期用的是LSTM，后来看到Transformer就用Transformer</li>
<li>改变一切的范式永远有个引擎，引擎能不断前进</li>
</ol>
<p>其近期的研究方向是提高模型的可靠性和可控性，加快模型从少量数据中学习知识的速度，并降低对人工指导的依赖，避免出现幻觉</p>
<h2 id="Why-OpenAI-Not-Google"><a href="#Why-OpenAI-Not-Google" class="headerlink" title="Why OpenAI Not Google"></a>Why OpenAI Not Google</h2><p>两者的技术人员可是差了一个数量级，OpenAI联合创始人Greg Brockman面对这个问题如此说道：</p>
<p>都是站在巨人的肩膀上，全AI行业在计算，算法，数据上都取得了进步，不过OpenAI在早期，做了一些深思熟虑的选择。</p>
<ul>
<li><p>第一个选择就是直面现实，很认真的想过如果想要在这个领域取得进展，需要做什么，也做了很多没用的尝试，才看到这些有用的结果</p>
</li>
<li><p>还有一点最重要的是让不同团队间可以紧密协作</p>
</li>
</ul>
<h2 id="Step-By-Step"><a href="#Step-By-Step" class="headerlink" title="Step By Step"></a>Step By Step</h2><h3 id="无心插柳"><a href="#无心插柳" class="headerlink" title="无心插柳"></a>无心插柳</h3><p>OpenAI过去有人尝试训练一个模型，预测亚马逊平台评论的下一个字符。最终得到了一个可以分析评论句法的模型，但同时也得到了一个达到SOTA的情绪分析分类器，可以告诉人们这条评论是好评还是差评。这个算法现在看来可能不足为奇，但在当时他们第一次从底层句法中分析出语义，他们就知道必须朝这个方向做下去。</p>
<blockquote>
<p>一些后来看上去很大的成果，都不是起初的目标，而是在实践过程中潜移默化发展出来的。</p>
</blockquote>
<h3 id="面对AI恐惧的解答"><a href="#面对AI恐惧的解答" class="headerlink" title="面对AI恐惧的解答"></a>面对AI恐惧的解答</h3><p>现在有很多对AI的担忧，呼吁暂停更强AI的研发。Brockman如此回应到：</p>
<ul>
<li>一开始考虑如何构建通用人工智能时，也是希望它能造福全人类。</li>
<li>不存在弄清楚所有安全性后再开始，这可能是对的，但他不知道该如何执行这一计划</li>
<li>唯一可行的是在机器变得完美之前，给人们时间来提建议。</li>
</ul>
<p>人类开发计算机，算法等技术，都是step by step，并要再推进的每一个阶段去弄清楚如何管理好它们，就好像养大一个孩子，是大家共同引导，给它树立规矩，而不是教它毁灭人类。</p>
<h1 id="成本与挑战"><a href="#成本与挑战" class="headerlink" title="成本与挑战"></a>成本与挑战</h1><p>当前能做ChatGPT这类事的机构，国外不超过5家，国内不超过3家</p>
<ul>
<li>Azure云服务为ChatGPT构建超过1w枚A100&#x2F;H100的计算集群，大型商业化后续投入还需更多</li>
<li>国内超过w枚GPU的企业不超过5家（高低配合起来）只有1家，有w枚A100的最多只有一家，短期内布局的选手十分有限。需要长期高成本投入，高性能GPU芯片短缺，采购成本和运营成本都很高昂，挑战的就是资金储备，战略意志和实际技术能力（含工程能力）。</li>
<li>考虑到成本问题，未来或许会出现股份制大模型，机构合作共建</li>
</ul>
<h2 id="智算集群成本"><a href="#智算集群成本" class="headerlink" title="智算集群成本"></a>智算集群成本</h2><ul>
<li>建设成本<ul>
<li>以A800 10w&#x2F;枚价格基准下，万枚采购成本10亿</li>
<li>一台服务器4-8枚GPU才经济，那就以40w一台GPU服务器来核算</li>
<li><font color="red">服务器采购成本通常是数据中心建设成本的30%</font>，那么这个智算集群建设成本通常超过30亿</li>
</ul>
</li>
<li>训练成本<ul>
<li>ChatGPT一次完整训练成本超过$1200w，差不多￥8000w，迭代10次完整训练，就有8亿支出</li>
<li>数据采集，人工标注等这些软性成本还难以简单计算</li>
</ul>
</li>
<li>运营成本<ul>
<li>网络带宽，电力资源，人员薪资，成本可能也是以亿计的</li>
</ul>
</li>
</ul>
<p>中短期无法盈利，用户规模越大，亏损可能也会越大，得输血支持。在22年财报上看，BAT中云指出56亿，266亿，311亿。百度可能财力上就无法支撑，战略意愿上因为与主营收模式冲突也会有持久性的问题。</p>
<blockquote>
<p>假设大厂50%的资本支出用于投资云基础设施（参照Amazon）</p>
</blockquote>
<h2 id="技术挑战"><a href="#技术挑战" class="headerlink" title="技术挑战"></a>技术挑战</h2><p>用GLM-130B参与者的话说，“预训练一个高精度的千亿模型与训练百亿模型完全不同”<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="GLM团队.[GLM-130B：开源的双语预训练模型](https://keg.cs.tsinghua.edu.cn/glm-130b/zh/posts/glm-130b/)
">[2]</span></a></sup>：</p>
<p>频繁的随机硬件故障、模型梯度爆炸、算法中意外的过多内存使用、新的 Megatron 和 DeepSpeed 框架中 3D 流水线的调试、无法从优化器状态中恢复、机器间 TCP 拥塞，以及许多许多意外的 “bug”，项目被多次推迟。</p>
<p>若不幸你们没有足够的训练资源，会遭遇到另一个难题：我们需要把训练代码适配到不同的硬件平台。不同的平台底层算子各不相同，很多算子还有所欠缺，还有阻碍收敛的各种问题，Softmax 和 Attention 的计算精度选择问题，还有你自己可能犯的各种错误，总之看看下面的清单，就知道是一个不是东风压倒西风，就是西风压倒东风的搏斗过程，是对组织能力和资源的一份挑战。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230516171310.png" style="zoom:40%;" alt="GLM-130B训练解决的技术问题"/>

<h1 id="冲击"><a href="#冲击" class="headerlink" title="冲击"></a>冲击</h1><p><font color="blur">社会性拐点已至，因为一项大型成本从边际变成固定，势必深刻变革各领域。</font></p>
<h2 id="奇点临近"><a href="#奇点临近" class="headerlink" title="奇点临近"></a>奇点临近</h2><p>从AI能力难度角度从低到高看</p>
<ol>
<li>简单聊天，事实性问题，写文章，写诗</li>
<li>简单计算，多轮对话</li>
<li>复杂指令，写代码</li>
<li>逻辑推理，复杂计算，事实一致性</li>
</ol>
<p>当前ChatGPT的表现上的确给力</p>
<ul>
<li>同一个模型完成各种开放任务，变成了通用任务助理，颠覆人类基本认知<ul>
<li>高质量对话让人误以为AI有意识和人格觉醒，产生数字生命的感觉<ul>
<li>模型和数据飞轮转的非常快，在很多考试领域已经超越大多数人类</li>
<li>人与AI共存的未来人类一直在畅想，机器人三定律1953年就提出来了</li>
</ul>
</li>
<li><font color="blue">人人都配有一个熟读人类知识的王语嫣</font>，当前你也可以说她不是真正学会了知识，学的是传载知识的语言搭配模式，但上下文理解能力和推理能力强，要是再配上人形机器人，那就不仅仅是个武功军师了。</li>
<li>以培养学习能力和创造能力为主，今后才好在竞争中更显突出。</li>
</ul>
</li>
<li>越大的机构，消耗在语言处理上的成本越高（信息协作），所以市场非常嗨<ul>
<li>从cv，音频这种感知智能上升到NLP到认知智能，再到更强大的AIGC。PGC -&gt; PGC+UGC -&gt; AIGC，内容生产门槛进一步降低，2025年AI生产内容可能站到所有的10%</li>
<li>白领工作在一轮生产力变革的前夜，知识密集型岗位的生产力变了，势必创造新的生产关系。<ul>
<li>关注&#x2F;反应最大的是知识生产&#x2F;知识密集型岗位，知识和技能平权进一步前进，影响稀缺性，互联网民工也有被替代的可能<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="稀土掘金.[Github Copilot 程序员下岗指南](https://juejin.cn/post/7218014583863656503)">[3]</span></a></sup></li>
<li>对记忆消耗的解放，可以让人们做更多的独立性思考</li>
<li>说的具体点，可能新闻，高等教育，图形，软件设计等行业的某些工作，有被AI替代的风险。金融行业里的许多岗位也会被裁掉，大学毕业后花两三年的时间像机器人一样做excel的工作，也是可以让人工智能来。但关键的金融和经济决策不会被机器替代</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="失业，预言还是谎言"><a href="#失业，预言还是谎言" class="headerlink" title="失业，预言还是谎言"></a>失业，预言还是谎言</h2><p><font color="blur">核心产业：科学，教育，医疗，这是OpenAI长期最关注的3个行业，也是整个社会最根本的。</font>（有些行业的生产资本本质是模型驱动，例如医疗就是一个模型行业，一个好医生是一个好模型）</p>
<p>机器人开始抢白领的工作，一般来说<u>贩卖焦虑的老套路都是用失业这个绝对痛点</u>，戳痛大家脆弱的神经，<font color="red">一焦虑你就得乖乖付费</font>。总之哪里有焦虑，哪里就有生意。</p>
<p>通过调查显示，从教育背景，工作经验，职业年限和工资数据来看，高薪水从业者更容易接触LLM，面临影响的风险更大。按行业来看，信息处理行业受到的影响较大，而制造业，农业和采矿业则表现出较低的影响风险。</p>
<p>现在ChatGPT引发的轰动，早期的搜索引擎也有过，你想想一个搜索框能告诉你所有问题的结果，这是一件多么可怕的事情，可后来的事情也很清楚。</p>
<blockquote>
<p><u><strong>论文库，各种教程，都是大杀器，放在封建社会都是要被统治阶级重兵把守的国家机密</strong></u>，如今无差别放在大家面前，<u><strong>问题是绝大部分人视而不见</strong></u>，如果之前那些东西并没有影响大家，一个chatgpt又有什么影响呢？</p>
</blockquote>
<ul>
<li>大概率一段喧闹后恢复平静，就像当初谷歌一样，<font color="blue">对绝大多数人只是提供了一点方便，小部分人觉得捡到了一把机枪 (变成少数人天天在用的工具，绝大多数人非必要不会去碰它)，社会差距会进一步拉大</font>，冲击的也是一小部分人</li>
<li>Excel出现的时候，很多人惊呼这玩意将改变整个职场江湖，谁能想到，它只是让大家的工作变得更琐碎了。</li>
<li>好处是工具的赋能，使人站的位置越来越高</li>
</ul>
<p>如果你面对的东西主观性很强，客户自己都不知道想要什么，或需要大量的想法，这种工作短期内AI还不太行，恰好这类工具不但不会取代你，且会成为你的帮手。</p>
<h3 id="正视人性"><a href="#正视人性" class="headerlink" title="正视人性"></a>正视人性</h3><blockquote>
<p>如<a href="https://blog.diffday.com/%E5%8E%86%E5%8F%B2%E7%9A%84%E6%95%99%E8%AE%AD.html">《历史的教训》</a>一书中提到的，人生来不自由不平等</p>
</blockquote>
<p>一些随手可通过搜索引擎查到的东西，绝大部分人却在那里疯狂传谣。同一个搜索框，不同的人查到的东西，差距都很大。现实世界里，80%的人是没有阅读长文的能力的，你再要求他们会使用复杂工具简直是为难大家了，太多人在强大工具面前就不知道该如何描述自己想要什么。</p>
<p><font color="purple">生活就像一个竞技场，每个人走到里面惊讶的发现里面摆着一堆武器，让大家自己选。这些武器从木棍到机枪应有尽有，令人不解的是，绝大部分人选的是操作简单容易上手的菜刀，而不是有一定学习成本的机枪。看似公平的竞争，最后因为工具的差别，变成了单方面的屠杀</font>。</p>
<blockquote>
<p>现实比较复杂一点，因为人不止一个工具，比如孩子比较蠢，选了木棍，而他爹有个高达。</p>
</blockquote>
<p>人类社会的大发展，回头看也不过百年，百年之间，人类文明早已经天翻地覆，但人类的天性和欲望并没有因此得到任何的进化和改变。</p>
<h2 id="拥抱未来"><a href="#拥抱未来" class="headerlink" title="拥抱未来"></a>拥抱未来</h2><p>过去学个知识，干一辈子的时代已经渐渐远去了，<font color="blue">经历了多次科技革命的我们，正处在一个加速时期</font>，新工具出现越来越快，取代效应也越来越快。</p>
<blockquote>
<p>大量受规训的人毕业了被告知还要再学习就情绪上抵触，好在社会教做人，因为很快意识到市场和工具变化究竟有多快。当然也有从事简单重复工作的岗位，与再学习逐渐分离，但多数也随之甩去了改变生活境遇层次的机会。</p>
</blockquote>
<p>电出来的时候被认为是会带来灾难的巫术，无论你是欣喜还是焦虑，它终究会在未来的某一天不期而遇。市场不会因为禁用而整体不用。</p>
<p>靠人口和房子的粗旷式发展的大周期已经结束，人口下滑也是不可逆的趋势，中国正在经历劳动密集向效率提升的转型。时代需要新科技，新动能来救场。</p>
<p><font color="blue">人类历史从来不是人和工具之间的搏斗，而是人+工具替代人的演变。</font>当人类整体内大幅增加时，个人优势被抹平，苦痛会随之重新增加，立于潮头，意味着更少的竞争与更多的机会。保持竞争优势，亦不要被欲望收割，才能获得轻松幸福的生活。</p>
<h3 id="教育适配"><a href="#教育适配" class="headerlink" title="教育适配"></a>教育适配</h3><p>我们小初中训练最多的死记硬背，心算，重复难度的刷题能力，这种反人性的规训是要进行反思的，不要成为一个按一定工序墨守成规的执行机器，这种能力20年后被人工智能淹没是大概率的事情。如何思考事物之间的关联，而不是只想快点看到老师的总结，面向未来学习。</p>
<p>在人工智能时代，能准确描述你要的东西，也变得非常有价值，美学的认知和表达能力成为一大要素，说到底我们是商品社会，未来大众会越来越为美的东西买单，如果制作过程不再那么重要，那么懂美学的孩子就能做出更出色的产品。</p>
<p>个性和特长的培养也会显得比以往更为重要（一直重要，但更为重要了）。新时代的动手能力，就是配合基础学科及美学素养，从小锻炼使用现代工具辅助学习的能力。</p>
<h1 id="中美AI研究差异"><a href="#中美AI研究差异" class="headerlink" title="中美AI研究差异"></a>中美AI研究差异</h1><blockquote>
<p>美国侧重基础研究，中国侧重解决方案。其实不仅AI，本世纪所有的科技发展，都在太平洋两岸衍生出不同的路径。</p>
</blockquote>
<ul>
<li>互联网浪潮美国对电商不热衷，线上消费渗透率一直上不去。中国几乎所有互联网公司都做过电商，渗透率冠绝全球，规模一度比2到11加总都高</li>
<li>移动互联网，中国凭借更好的网络环境，更鼓励创新的监管制度，直接跳过信用卡时代，进入数字支付时代</li>
<li>无人驾驶，美国侧重车的智能化，中国有更好的基建，路况，网络和交通规划，于是选了车路协同的路线</li>
<li>产业互联网，美国经济产业特点处于微笑曲线的两头，科技，互联网，金融占比高，加上人力昂贵，企业付费意愿强。中国集中在微笑曲线中段，作为世界工厂，场景丰富，产业链完整，政策支持，高效集中，产学研对接十分方便，技术验证更好落地。这样的大背景导致美国重攻基础研究，多是从技术起步，<font color="blue">中国优势在于场景多，需求多，往往是场景倒逼技术落地</font>。</li>
</ul>
<p><font color="blue"><u>中国民营企业才刚从艰苦奋斗的路上走出来，精打细算的习惯改变不了</u>，往往从市场需求产品需求开始，再慢慢投入科学家和基础研究，带动落地</font>。<font color="red">美国巨头钱不是问题，钱太多才是问题，砸钱做基础科学，既可以抢占科技高地，也需要冲淡垄断者的坏形象。</font></p>
<p>美国AI行业上一个爆款DeepMind的Alpha系列，就是先把技术做出来，赢围棋冠军，但商业落地慢慢探索，好几年后这项技术被用于破解蛋白质折叠结构难题，参与新药研发，才算英雄有用武之地。</p>
<p>中国用户早期很多用个人电脑自拍QQ头像，QQ团队就想，做个技术实现头像居中，解决这个问题后，逐步孵化出人脸检测，人像表情，智能P图等技术。用回产品，孵化出天天P图；人像美容技术再用到全民K歌，这个图像团队就是腾讯优图。还有美团的无人机，京东的智能供应链，都市跟主业投入有关。</p>
<h2 id="欧洲在哪"><a href="#欧洲在哪" class="headerlink" title="欧洲在哪?"></a>欧洲在哪?</h2><p>一句戏谑：美国人在创新，中国人在应用，欧洲人在立法。例如大模型商用基本只剩中美两个玩家。</p>
<p>当然中国当前一些科技领域也走在世界探索的前列，相对而言美国还是更强。</p>
<h2 id="一个词话三方"><a href="#一个词话三方" class="headerlink" title="一个词话三方"></a>一个词话三方</h2><p>美国<font color="blur"><strong>虎</strong></font>（0-1，无知者无畏的自信去行动，想到就真敢去做），中国<font color="blur"><strong>卷</strong></font>（1-100），欧洲<font color="blur"><strong>守</strong></font>（保守高傲和磨叽）</p>
<h1 id="重要参考"><a href="#重要参考" class="headerlink" title="重要参考"></a>重要参考</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">张俊林.<a target="_blank" rel="noopener" href="http://k.sina.com.cn/article_2674405451_9f68304b01901346f.html">由ChatGPT反思大语言模型（LLM）的技术精要</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">GLM团队.<a target="_blank" rel="noopener" href="https://keg.cs.tsinghua.edu.cn/glm-130b/zh/posts/glm-130b/">GLM-130B：开源的双语预训练模型</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">稀土掘金.<a target="_blank" rel="noopener" href="https://juejin.cn/post/7218014583863656503">Github Copilot 程序员下岗指南</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://blog.diffday.com">DiffDay</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.diffday.com/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">https://blog.diffday.com/大语言模型.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.diffday.com" target="_blank">Diffday</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a></div><div class="post_share"><div class="social-share" data-image="http://cdn.diffday.com/picgo/20230408222056.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="http://cdn.diffday.com/picgo/wechatpay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/wechatpay.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="http://cdn.diffday.com/picgo/alipay-n.jpeg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/alipay-n.jpeg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E8%87%AA%E5%BE%8Bvs%E8%87%AA%E7%94%B1.html" title="自律是一种阶级属性"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230330165007.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">自律是一种阶级属性</div></div></a></div><div class="next-post pull-right"><a href="/DevSecOps.html" title="DevSecOps"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230315140348.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">DevSecOps</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/DevSecOps.html" title="DevSecOps"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230315140348.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-15</div><div class="title">DevSecOps</div></div></a></div><div><a href="/Serverless.html" title="Serverless简解"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20221208174734.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-08</div><div class="title">Serverless简解</div></div></a></div><div><a href="/%E5%A0%86%E6%8E%92%E5%BA%8F%E4%B8%8E%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F.html" title="堆排序与归并排序"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20221024110942.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-16</div><div class="title">堆排序与归并排序</div></div></a></div><div><a href="/%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%E5%8A%A0%E5%AF%86.html" title="敏感信息加密"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230314172247.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-14</div><div class="title">敏感信息加密</div></div></a></div><div><a href="/%E6%95%B0%E5%AD%97%E5%8C%96%E6%97%B6%E4%BB%A3%E5%9B%9E%E7%9C%8B%E8%BF%90%E7%BB%B4.html" title="数字化时代回看运维"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20230526183811.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-26</div><div class="title">数字化时代回看运维</div></div></a></div><div><a href="/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E6%B5%85%E8%BF%B0.html" title="数据倾斜浅述"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://cdn.diffday.com/picgo/20221109111839.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-10</div><div class="title">数据倾斜浅述</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%83%8A%E5%96%9C%E4%B8%8E%E6%83%8A%E9%86%92"><span class="toc-number">1.</span> <span class="toc-text">惊喜与惊醒</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NLP%E7%A0%94%E7%A9%B6%E8%8C%83%E5%BC%8F%E8%BD%AC%E5%8F%98"><span class="toc-number">2.</span> <span class="toc-text">NLP研究范式转变</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E4%B8%A4%E9%98%B6%E6%AE%B5%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">从深度学习到两阶段训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%9F"><span class="toc-number">2.1.1.</span> <span class="toc-text">深度学习期</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E9%98%B6%E6%AE%B5%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.2.</span> <span class="toc-text">两阶段训练大模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E6%9D%A5%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">2.1.3.</span> <span class="toc-text">带来的影响</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%AD%E9%97%B4%E4%BB%BB%E5%8A%A1%E6%B6%88%E4%BA%A1"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">中间任务消亡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF%E7%BB%9F%E4%B8%80"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">技术路线统一</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E5%88%B0%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="toc-number">2.2.</span> <span class="toc-text">预训练到通用人工智能</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ChatGPT"><span class="toc-number">2.2.1.</span> <span class="toc-text">ChatGPT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ChatGPT%E6%94%B9%E5%8F%98%E4%BA%86GPT-3-5%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">ChatGPT改变了GPT-3.5什么？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM%E7%9A%84%E7%9F%A5%E8%AF%86%E6%9E%84%E6%88%90"><span class="toc-number">2.2.2.</span> <span class="toc-text">LLM的知识构成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AD%98%E5%8F%96%E7%9F%A5%E8%AF%86"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">如何存取知识</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM%E7%9A%84%E8%A7%84%E6%A8%A1%E6%95%88%E5%BA%94"><span class="toc-number">2.2.3.</span> <span class="toc-text">LLM的规模效应</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E7%9A%84%E7%A8%80%E7%96%8F%E5%8C%96"><span class="toc-number">2.2.4.</span> <span class="toc-text">Transformer的稀疏化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92"><span class="toc-number">2.2.5.</span> <span class="toc-text">人机交互</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%A2%9E%E5%BC%BALLM%E7%9A%84%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B"><span class="toc-number">2.2.6.</span> <span class="toc-text">如何增强LLM的推理能力</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%97%E5%8A%9B%E7%BA%A6%E6%9D%9F%E4%B8%8B%E7%9A%84%E6%9C%80%E4%BC%98%E5%9F%B9%E5%85%BB%E7%AD%96%E7%95%A5"><span class="toc-number">3.</span> <span class="toc-text">算力约束下的最优培养策略</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%BA%E4%BD%95%E6%98%AFOpenAI"><span class="toc-number">4.</span> <span class="toc-text">为何是OpenAI</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-OpenAI-Not-Google"><span class="toc-number">4.1.</span> <span class="toc-text">Why OpenAI Not Google</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Step-By-Step"><span class="toc-number">4.2.</span> <span class="toc-text">Step By Step</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E5%BF%83%E6%8F%92%E6%9F%B3"><span class="toc-number">4.2.1.</span> <span class="toc-text">无心插柳</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%A2%E5%AF%B9AI%E6%81%90%E6%83%A7%E7%9A%84%E8%A7%A3%E7%AD%94"><span class="toc-number">4.2.2.</span> <span class="toc-text">面对AI恐惧的解答</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%88%90%E6%9C%AC%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-number">5.</span> <span class="toc-text">成本与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%99%BA%E7%AE%97%E9%9B%86%E7%BE%A4%E6%88%90%E6%9C%AC"><span class="toc-number">5.1.</span> <span class="toc-text">智算集群成本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E6%8C%91%E6%88%98"><span class="toc-number">5.2.</span> <span class="toc-text">技术挑战</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B2%E5%87%BB"><span class="toc-number">6.</span> <span class="toc-text">冲击</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A5%87%E7%82%B9%E4%B8%B4%E8%BF%91"><span class="toc-number">6.1.</span> <span class="toc-text">奇点临近</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%B1%E4%B8%9A%EF%BC%8C%E9%A2%84%E8%A8%80%E8%BF%98%E6%98%AF%E8%B0%8E%E8%A8%80"><span class="toc-number">6.2.</span> <span class="toc-text">失业，预言还是谎言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E8%A7%86%E4%BA%BA%E6%80%A7"><span class="toc-number">6.2.1.</span> <span class="toc-text">正视人性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%A5%E6%8A%B1%E6%9C%AA%E6%9D%A5"><span class="toc-number">6.3.</span> <span class="toc-text">拥抱未来</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%99%E8%82%B2%E9%80%82%E9%85%8D"><span class="toc-number">6.3.1.</span> <span class="toc-text">教育适配</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%AD%E7%BE%8EAI%E7%A0%94%E7%A9%B6%E5%B7%AE%E5%BC%82"><span class="toc-number">7.</span> <span class="toc-text">中美AI研究差异</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AC%A7%E6%B4%B2%E5%9C%A8%E5%93%AA"><span class="toc-number">7.1.</span> <span class="toc-text">欧洲在哪?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E8%AF%8D%E8%AF%9D%E4%B8%89%E6%96%B9"><span class="toc-number">7.2.</span> <span class="toc-text">一个词话三方</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E5%8F%82%E8%80%83"><span class="toc-number">8.</span> <span class="toc-text">重要参考</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2014 - 2023 By DiffDay</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/"><img class="icp-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src=""><span>粤ICP备18103410号</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  //if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addModeChange('mermaid', runMermaid)

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>